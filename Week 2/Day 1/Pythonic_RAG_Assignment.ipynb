{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lElF3o5PR6ys"
      },
      "source": [
        "# Your First RAG Application\n",
        "\n",
        "In this notebook, we'll walk you through each of the components that are involved in a simple RAG application.\n",
        "\n",
        "We won't be leveraging any fancy tools, just the OpenAI Python SDK, Numpy, and some classic Python.\n",
        "\n",
        "> NOTE: This was done with Python 3.11.4.\n",
        "\n",
        "> NOTE: There might be [compatibility issues](https://github.com/wandb/wandb/issues/7683) if you're on NVIDIA driver >552.44 As an interim solution - you can rollback your drivers to the 552.44."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CtcL8P8R6yt"
      },
      "source": [
        "## Table of Contents:\n",
        "\n",
        "- Task 1: Imports and Utilities\n",
        "- Task 2: Documents\n",
        "- Task 3: Embeddings and Vectors\n",
        "- Task 4: Prompts\n",
        "- Task 5: Retrieval Augmented Generation\n",
        "  - 🚧 Activity #1: Augment RAG\n",
        "- Task 6: Visibility Tooling\n",
        "- Task 7: RAG Evaluation Using GPT-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Dz6GYilR6yt"
      },
      "source": [
        "Let's look at a rather complicated looking visual representation of a basic RAG application.\n",
        "\n",
        "<img src=\"https://i.imgur.com/PvlaIUO.png\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjmC0KFtR6yt"
      },
      "source": [
        "## Task 1: Imports and Utility\n",
        "\n",
        "We're just doing some imports and enabling `async` to work within the Jupyter environment here, nothing too crazy!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from rich import print\n",
        "%load_ext rich"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7VEzqziR6yt",
        "outputId": "f873dd3b-55a0-4e00-ecf4-e2a0fe3af327"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: Failed to remove contents in a temporary directory 'E:\\Projects\\AI-Maven\\AIE3\\.venv\\Lib\\site-packages\\~umpy'.\n",
            "  You can safely remove it manually.\n"
          ]
        }
      ],
      "source": [
        "# !pip install -qU numpy matplotlib plotly pandas scipy scikit-learn openai python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Z1dyrG4hR6yt"
      },
      "outputs": [],
      "source": [
        "from aimakerspace.text_utils import TextFileLoader, CharacterTextSplitter\n",
        "from aimakerspace.vectordatabase import VectorDatabase\n",
        "import asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9OrFZRnER6yt"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0jGnpQsR6yu"
      },
      "source": [
        "## Task 2: Documents\n",
        "\n",
        "We'll be concerning ourselves with this part of the flow in the following section:\n",
        "\n",
        "<img src=\"https://i.imgur.com/jTm9gjk.png\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SFPWvRUR6yu"
      },
      "source": [
        "### Loading Source Documents\n",
        "\n",
        "So, first things first, we need some documents to work with.\n",
        "\n",
        "While we could work directly with the `.txt` files (or whatever file-types you wanted to extend this to) we can instead do some batch processing of those documents at the beginning in order to store them in a more machine compatible format.\n",
        "\n",
        "In this case, we're going to parse our text file into a single document in memory.\n",
        "\n",
        "Let's look at the relevant bits of the `TextFileLoader` class:\n",
        "\n",
        "```python\n",
        "def load_file(self):\n",
        "        with open(self.path, \"r\", encoding=self.encoding) as f:\n",
        "            self.documents.append(f.read())\n",
        "```\n",
        "\n",
        "We're simply loading the document using the built in `open` method, and storing that output in our `self.documents` list.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia2sUEuGR6yu",
        "outputId": "84937ecc-c35f-4c4a-a4ab-9da72625954c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "\u001b[1;36m1\u001b[0m"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_loader = TextFileLoader(\"data/PMarcaBlogs.txt\")\n",
        "documents = text_loader.load_documents()\n",
        "len(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV-tj5WFR6yu",
        "outputId": "674eb315-1ff3-4597-bcf5-38ece0a812ac"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">﻿\n",
              "The Pmarca Blog Archives\n",
              "<span style=\"font-weight: bold\">(</span>select posts from <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2007</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2009</span><span style=\"font-weight: bold\">)</span>\n",
              "Marc Andreessen\n",
              "copyright: Andreessen Horow\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "The Pmarca Blog Archives\n",
              "\u001b[1m(\u001b[0mselect posts from \u001b[1;36m2007\u001b[0m-\u001b[1;36m2009\u001b[0m\u001b[1m)\u001b[0m\n",
              "Marc Andreessen\n",
              "copyright: Andreessen Horow\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(documents[0][:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHlTvCzYR6yu"
      },
      "source": [
        "### Splitting Text Into Chunks\n",
        "\n",
        "As we can see, there is one document - and it's the entire text of Frakenstein\n",
        "\n",
        "We'll want to chunk the document into smaller parts so it's easier to pass the most relevant snippets to the LLM.\n",
        "\n",
        "There is no fixed way to split/chunk documents - and you'll need to rely on some intuition as well as knowing your data *very* well in order to build the most robust system.\n",
        "\n",
        "For this toy example, we'll just split blindly on length.\n",
        "\n",
        ">There's an opportunity to clear up some terminology here, for this course we will be stick to the following:\n",
        ">\n",
        ">- \"source documents\" : The `.txt`, `.pdf`, `.html`, ..., files that make up the files and information we start with in its raw format\n",
        ">- \"document(s)\" : single (or more) text object(s)\n",
        ">- \"corpus\" : the combination of all of our documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0DV5Yx5R6yu"
      },
      "source": [
        "Let's take a peek visually at what we're doing here - and why it might be useful:\n",
        "\n",
        "<img src=\"https://i.imgur.com/rtM6Ci6.png\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G6Voc0jR6yv"
      },
      "source": [
        "As you can see (though it's not specifically true in this toy example) the idea of splitting documents is to break them into managable sized chunks that retain the most relevant local context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMC4tsEmR6yv",
        "outputId": "08689c0b-57cd-4040-942a-8193e997f5cb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "\u001b[1;36m373\u001b[0m"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_splitter = CharacterTextSplitter()\n",
        "split_documents = text_splitter.split_texts(documents)\n",
        "len(split_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2wKT0WLR6yv"
      },
      "source": [
        "Let's take a look at some of the documents we've managed to split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcYMwWJoR6yv",
        "outputId": "20d69876-feca-4826-b4be-32915276987a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'\\ufeff\\nThe Pmarca Blog Archives\\n(select posts from 2007-2009)\\nMarc Andreessen\\ncopyright: Andreessen </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Horowitz\\ncover design: Jessica Hagy\\nproduced using: Pressbooks\\nContents\\nTHE PMARCA GUIDE TO STARTUPS\\nPart 1: </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Why not to do a startup 2\\nPart 2: When the VCs say \"no\" 10\\nPart 3: \"But I don\\'t know any VCs!\" 18\\nPart 4: The </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">only thing that matters 25\\nPart 5: The Moby Dick theory of big companies 33\\nPart 6: How much funding is too </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">little? Too much? 41\\nPart 7: Why a startup\\'s initial business plan doesn\\'t\\nmatter that much\\n49\\nTHE PMARCA </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">GUIDE TO HIRING\\nPart 8: Hiring, managing, promoting, and Dring\\nexecutives\\n54\\nPart 9: How to hire a professional</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">CEO 68\\nHow to hire the best people you\\'ve ever worked\\nwith\\n69\\nTHE PMARCA GUIDE TO BIG COMPANIES\\nPart 1: </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Turnaround! 82\\nPart 2: Retaining great people 86\\nTHE PMARCA GUIDE TO CAREER, PRODUCTIVITY,\\nAND SOME OTHER </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">THINGS\\nIntroduction 97\\nPart 1: Opportunity 99\\nPart 2: Skills and education 107\\nPart 3: Where to go and why </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">120\\nThe Pmarca Guide to Personal Productivi'</span>\n",
              "<span style=\"font-weight: bold\">]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m[\u001b[0m\n",
              "    \u001b[32m'\\ufeff\\nThe Pmarca Blog Archives\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mselect posts from 2007-2009\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nMarc Andreessen\\ncopyright: Andreessen \u001b[0m\n",
              "\u001b[32mHorowitz\\ncover design: Jessica Hagy\\nproduced using: Pressbooks\\nContents\\nTHE PMARCA GUIDE TO STARTUPS\\nPart 1: \u001b[0m\n",
              "\u001b[32mWhy not to do a startup 2\\nPart 2: When the VCs say \"no\" 10\\nPart 3: \"But I don\\'t know any VCs!\" 18\\nPart 4: The \u001b[0m\n",
              "\u001b[32monly thing that matters 25\\nPart 5: The Moby Dick theory of big companies 33\\nPart 6: How much funding is too \u001b[0m\n",
              "\u001b[32mlittle? Too much? 41\\nPart 7: Why a startup\\'s initial business plan doesn\\'t\\nmatter that much\\n49\\nTHE PMARCA \u001b[0m\n",
              "\u001b[32mGUIDE TO HIRING\\nPart 8: Hiring, managing, promoting, and Dring\\nexecutives\\n54\\nPart 9: How to hire a professional\u001b[0m\n",
              "\u001b[32mCEO 68\\nHow to hire the best people you\\'ve ever worked\\nwith\\n69\\nTHE PMARCA GUIDE TO BIG COMPANIES\\nPart 1: \u001b[0m\n",
              "\u001b[32mTurnaround! 82\\nPart 2: Retaining great people 86\\nTHE PMARCA GUIDE TO CAREER, PRODUCTIVITY,\\nAND SOME OTHER \u001b[0m\n",
              "\u001b[32mTHINGS\\nIntroduction 97\\nPart 1: Opportunity 99\\nPart 2: Skills and education 107\\nPart 3: Where to go and why \u001b[0m\n",
              "\u001b[32m120\\nThe Pmarca Guide to Personal Productivi'\u001b[0m\n",
              "\u001b[1m]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(split_documents[0:1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOU-RFP_R6yv"
      },
      "source": [
        "## Task 3: Embeddings and Vectors\n",
        "\n",
        "Next, we have to convert our corpus into a \"machine readable\" format.\n",
        "\n",
        "Loosely, this means turning the text into numbers.\n",
        "\n",
        "Today, we're going to talk about the actual process of creating, and then storing, these embeddings, and how we can leverage that to intelligently add context to our queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Embeddings and Dense Vector Search: A Quick Primer\n",
        "\n",
        "If you come from an NLP background, embeddings are something you might be intimately familiar with - otherwise, you might find the topic a bit...dense. (this attempt at a joke will make more sense later)\n",
        "\n",
        "In all seriousness, embeddings are a powerful piece of the NLP puzzle, so let's dive in!\n",
        "\n",
        "> NOTE: While this notebook language/NLP-centric, embeddings have uses beyond just text!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Why Do We Even Need Embeddings?\n",
        "\n",
        "In order to fully understand what Embeddings are, we first need to understand why we have them!\n",
        "\n",
        "Machine Learning algorithms, ranging from the very big to the very small, all have one thing in common:\n",
        "\n",
        "They need numeric inputs.\n",
        "\n",
        "So we need a process by which to translate the domain we live in, dominated by images, audio, language, and more, into the domain of the machine: Numbers.\n",
        "\n",
        "Another thing we want to be able to do is capture \"semantic information\" about words/phrases so that we can use algorithmic approaches to determine if words are closely related or not!\n",
        "\n",
        "So, we need to come up with a process that does these two things well:\n",
        "\n",
        "- Convert non-numeric data into numeric-data\n",
        "- Capture potential semantic relationships between individual pieces of data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### How Do Embeddings Capture Semantic Relationships?\n",
        "\n",
        "In a simplified sense, embeddings map a word or phrase into n-dimensional space with a dense continuous vector, where each dimension in the vector represents some \"latent feature\" of the data.\n",
        "\n",
        "This is best represented in a classic example:\n",
        "\n",
        "![image](https://i.imgur.com/K5eQtmH.png)\n",
        "\n",
        "As can be seen in the extremely simplified example: The X_1 axis represents age, and the X_2 axis represents hair.\n",
        "\n",
        "The relationship of \"puppy -> dog\" reflects the same relationship as \"baby -> adult\", but dogs are (typically) hairier than humans. However, adults typically have more hair than babies - so they are shifted slightly closer to dogs on the X_2 axis!\n",
        "\n",
        "Now, this is a simplified and contrived example - but it is *essentially* the mechanism by which embeddings capture semantic information.\n",
        "\n",
        "In reality, the dimensions don't sincerely represent hard-concepts like \"age\" or \"hair\", but it's useful as a way to think about how the semantic relationships are captured.\n",
        "\n",
        "Alright, with some history behind us - let's examine how these might help us choose relevant context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's begin with a simple example - simply looking at how close to embedding vectors are for a given phrase.\n",
        "\n",
        "When we use the term \"close\" in this notebook - we're referring to a distance measure called \"cosine similarity\".\n",
        "\n",
        "We discussed above that if two embeddings are close - they are semantically similar, cosine similarity gives us a quick way to measure how similar two vectors are!\n",
        "\n",
        "Closeness is measured from 1 to -1, with 1 being extremely close and -1 being extremely close to opposite in meaning.\n",
        "\n",
        "Let's implement it with Numpy below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "\n",
        "def cosine_similarity(vec_1, vec_2):\n",
        "    return np.dot(vec_1, vec_2) / (norm(vec_1) * norm(vec_2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's use the `text-embedding-3-small` embedding model (more on that in a second) to embed two sentences. In order to use this embedding model endpoint - we'll need to provide our OpenAI API key!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaOETZGpR6yv",
        "outputId": "1239abf1-faff-49f2-a67c-7350e50fb1b9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "assert os.environ.get(\"OPENAI_API_KEY\") is not None, \"Please set the OPENAI_API_KEY environment variable.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from aimakerspace.openai_utils.embedding import EmbeddingModel\n",
        "\n",
        "embedding_model = EmbeddingModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define our two sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "puppy_sentence = \"I love puppies!\"\n",
        "dog_sentence = \"I love dogs!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can convert those into embedding vectors using OpenAI!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "puppy_vector = embedding_model.get_embedding(puppy_sentence)\n",
        "dog_vector = embedding_model.get_embedding(dog_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can determine how closely they are related using our distance measure!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "\u001b[1;36m0.8340837865304538\u001b[0m"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cosine_similarity(puppy_vector, dog_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remember, with cosine similarity, close to 1. means they're very close!\n",
        "\n",
        "Let's see what happens if we use a different set of sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "\u001b[1;36m0.37241751325248423\u001b[0m"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "puppy_sentence = \"I love puppies!\"\n",
        "cat_sentence = \"I dislike cats!\"\n",
        "\n",
        "puppy_vector = embedding_model.get_embedding(puppy_sentence)\n",
        "cat_vector = embedding_model.get_embedding(cat_sentence)\n",
        "\n",
        "cosine_similarity(puppy_vector, cat_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see - these vectors are further apart - as expected!\n",
        "\n",
        "Now that we've gotten some background - lets see this put together with a few extra layers on top!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vector Database\n",
        "\n",
        "Let's set up our vector database to hold all our documents and their embeddings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDQrfAR1R6yv"
      },
      "source": [
        "While this is all baked into 1 call - we can look at some of the code that powers this process to get a better understanding:\n",
        "\n",
        "Let's look at our `VectorDatabase().__init__()`:\n",
        "\n",
        "```python\n",
        "def __init__(self, embedding_model: EmbeddingModel = None):\n",
        "        self.vectors = defaultdict(np.array)\n",
        "        self.embedding_model = embedding_model or EmbeddingModel()\n",
        "```\n",
        "\n",
        "As you can see - our vectors are merely stored as a dictionary of `np.array` objects.\n",
        "\n",
        "Secondly, our `VectorDatabase()` has a default `EmbeddingModel()` which is a wrapper for OpenAI's `text-embedding-3-small` model.\n",
        "\n",
        "> **Quick Info About `text-embedding-3-small`**:\n",
        "> - It has a context window of **8191** tokens\n",
        "> - It returns vectors with dimension **1536**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L273pRdeR6yv"
      },
      "source": [
        "#### ❓Question #1:\n",
        "\n",
        "The default embedding dimension of `text-embedding-3-small` is 1536, as noted above. \n",
        "\n",
        "1. Is there any way to modify this dimension?\n",
        "2. What technique does OpenAI use to achieve this?\n",
        "\n",
        "> NOTE: Check out this [API documentation](https://platform.openai.com/docs/api-reference/embeddings/create) for the answer to question #1, and [this documentation](https://platform.openai.com/docs/guides/embeddings/use-cases) for an answer to question #2!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Answer #1:\n",
        "The dimension can be modified for the `text-embedding-3` models and newer by specifying the `dimensions` parameter as an integer. This is possible because of the following training techqniue: [Matryoshka Representation Learning](https://arxiv.org/abs/2205.13147)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5FZY7K3R6yv"
      },
      "source": [
        "We can call the `async_get_embeddings` method of our `EmbeddingModel()` on a list of `str` and receive a list of `float` back!\n",
        "\n",
        "```python\n",
        "async def async_get_embeddings(self, list_of_text: List[str]) -> List[List[float]]:\n",
        "        return await aget_embeddings(\n",
        "            list_of_text=list_of_text, engine=self.embeddings_model_name\n",
        "        )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSct6X0aR6yv"
      },
      "source": [
        "We cast those to `np.array` when we build our `VectorDatabase()`:\n",
        "\n",
        "```python\n",
        "async def abuild_from_list(self, list_of_text: List[str]) -> \"VectorDatabase\":\n",
        "        embeddings = await self.embedding_model.async_get_embeddings(list_of_text)\n",
        "        for text, embedding in zip(list_of_text, embeddings):\n",
        "            self.insert(text, np.array(embedding))\n",
        "        return self\n",
        "```\n",
        "\n",
        "And that's all we need to do!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "O4KoLbVDR6yv"
      },
      "outputs": [],
      "source": [
        "vector_db = VectorDatabase()\n",
        "vector_db = asyncio.run(vector_db.abuild_from_list(split_documents))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "\u001b[1;36m373\u001b[0m"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vector_db.vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSZwaGvpR6yv"
      },
      "source": [
        "#### ❓Question #2:\n",
        "\n",
        "What are the benefits of using an `async` approach to collecting our embeddings?\n",
        "\n",
        "> NOTE: Determining the core difference between `async` and `sync` will be useful! If you get stuck - ask ChatGPT!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Answer #2:\n",
        "The core benefit of using an `async` approach is that it allows for non-blocking operations. This means that while one operation is waiting for a response, other operations can be executed. In the above context, this allows us to add vector embeddings to the database as they are processed, instead of waiting for all or a subset of the embeddings to be processed before adding them to the database. \n",
        "\n",
        "This allows for a more efficient use of resources and can speed up the overall process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRBdIt-xR6yw"
      },
      "source": [
        "So, to review what we've done so far in natural language:\n",
        "\n",
        "1. We load source documents\n",
        "2. We split those source documents into smaller chunks (documents)\n",
        "3. We send each of those documents to the `text-embedding-3-small` OpenAI API endpoint\n",
        "4. We store each of the text representations with the vector representations as keys/values in a dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-vWANZyR6yw"
      },
      "source": [
        "### Semantic Similarity\n",
        "\n",
        "The next step is to be able to query our `VectorDatabase()` with a `str` and have it return to us vectors and text that is most relevant from our corpus.\n",
        "\n",
        "We're going to use the following process to achieve this in our toy example:\n",
        "\n",
        "1. We need to embed our query with the same `EmbeddingModel()` as we used to construct our `VectorDatabase()`\n",
        "2. We loop through every vector in our `VectorDatabase()` and use a distance measure to compare how related they are\n",
        "3. We return a list of the top `k` closest vectors, with their text representations\n",
        "\n",
        "There's some very heavy optimization that can be done at each of these steps - but let's just focus on the basic pattern in this notebook.\n",
        "\n",
        "> We are using [cosine similarity](https://www.engati.com/glossary/cosine-similarity) as a distance metric in this example - but there are many many distance metrics you could use - like [these](https://flavien-vidal.medium.com/similarity-distances-for-natural-language-processing-16f63cd5ba55)\n",
        "\n",
        "> We are using a rather inefficient way of calculating relative distance between the query vector and all other vectors - there are more advanced approaches that are much more efficient, like [ANN](https://towardsdatascience.com/comprehensive-guide-to-approximate-nearest-neighbors-algorithms-8b94f057d6b6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76d96uavR6yw",
        "outputId": "bbfccc31-20a2-41c7-c14d-46554a43ed2d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m[\u001b[0m\n",
              "    \u001b[1m(\u001b[0m\n",
              "        \u001b[32m'ordingly.\\nSeventh, when hiring the executive to run your former specialty, be\\ncareful you don’t hire someone weak on purpose.\\nThis sounds silly, but you wouldn’t believe how oaen it happens.\\nThe CEO who used to be a product manager who has a weak\\nproduct management executive. The CEO who used to be in\\nsales who has a weak sales executive. The CEO who used to be\\nin marketing who has a weak marketing executive.\\nI call this the “Michael Eisner Memorial Weak Executive Problem” — aaer the CEO of Disney who had previously been a brilliant TV network executive. When he bought ABC at Disney, it\\npromptly fell to fourth place. His response? “If I had an extra\\ntwo days a week, I could turn around ABC myself.” Well, guess\\nwhat, he didn’t have an extra two days a week.\\nA CEO — or a startup founder — oaen has a hard time letting\\ngo of the function that brought him to the party. The result: you\\nhire someone weak into the executive role for that function so\\nthat you can continue to be “the man” — cons'\u001b[0m,\n",
              "        \u001b[1;36m0.6539043027545371\u001b[0m\n",
              "    \u001b[1m)\u001b[0m,\n",
              "    \u001b[1m(\u001b[0m\n",
              "        \u001b[32m'm. They have areas where they are truly deXcient in judgment or skill set. That’s just life. Almost nobody is brilliant\\nat everything. When hiring and when Hring executives, you\\nmust therefore focus on strength rather than lack of weakness. Everybody has severe weaknesses even if you can’t see\\nthem yet. When managing, it’s oaen useful to micromanage and\\nto provide remedial training around these weaknesses. Doing so\\nmay make the diWerence between an executive succeeding or\\nfailing.\\nFor example, you might have a brilliant engineering executive\\nwho generates excellent team loyalty, has terriXc product judgment and makes the trains run on time. This same executive\\nmay be very poor at relating to the other functions in the company. She may generate far more than her share of cross-functional conYicts, cut herself oW from critical information, and\\nsigniXcantly impede your ability to sell and market eWectively.\\nYour alternatives are:\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32ma\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Macro-manage and give her an annual or quarterly object'\u001b[0m,\n",
              "        \u001b[1;36m0.5036247837648782\u001b[0m\n",
              "    \u001b[1m)\u001b[0m,\n",
              "    \u001b[1m(\u001b[0m\n",
              "        \u001b[32m'ed?\\nIn reality — as opposed to Marc’s warped view of reality — it will\\nbe extremely helpful for Marc \u001b[0m\u001b[32m[\u001b[0m\u001b[32mif he were actually the CEO,\\nwhich he is not\u001b[0m\u001b[32m]\u001b[0m\u001b[32m to meet with the new head of engineering daily\\nwhen she comes on board and review all of her thinking and\\ndecisions. This level of micromanagement will accelerate her\\ntraining and improve her long-term eWectiveness. It will make\\nher seem smarter to the rest of the organization which will build\\ncredibility and conXdence while she comes up to speed. Micromanaging new executives is generally a good idea for a limited\\nperiod of time.\\nHowever, that is not the only time that it makes sense to micro66 The Pmarca Blog Archives\\nmanage executives. It turns out that just about every executive\\nin the world has a few things that are seriously wrong with\\nthem. They have areas where they are truly deXcient in judgment or skill set. That’s just life. Almost nobody is brilliant\\nat everything. When hiring and when Hring executives, you\\nmust therefore focus o'\u001b[0m,\n",
              "        \u001b[1;36m0.4814861061791066\u001b[0m\n",
              "    \u001b[1m)\u001b[0m\n",
              "\u001b[1m]\u001b[0m"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_db.search_by_text(\n",
        "    \"What is the Michael Eisner Memorial Weak Executive Problem?\", k=3\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TehsfIiKR6yw"
      },
      "source": [
        "## Task 4: Prompts\n",
        "\n",
        "In the following section, we'll be looking at the role of prompts - and how they help us to guide our application in the right direction.\n",
        "\n",
        "In this notebook, we're going to rely on the idea of \"zero-shot in-context learning\".\n",
        "\n",
        "This is a lot of words to say: \"We will ask it to perform our desired task in the prompt, and provide no examples.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXpA0UveR6yw"
      },
      "source": [
        "### XYZRolePrompt\n",
        "\n",
        "Before we do that, let's stop and think a bit about how OpenAI's chat models work.\n",
        "\n",
        "We know they have roles - as is indicated in the following API [documentation](https://platform.openai.com/docs/api-reference/chat/create#chat/create-messages)\n",
        "\n",
        "There are three roles, and they function as follows (taken directly from [OpenAI](https://platform.openai.com/docs/guides/gpt/chat-completions-api)):\n",
        "\n",
        "- `{\"role\" : \"system\"}` : The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. However note that the system message is optional and the model’s behavior without a system message is likely to be similar to using a generic message such as \"You are a helpful assistant.\"\n",
        "- `{\"role\" : \"user\"}` : The user messages provide requests or comments for the assistant to respond to.\n",
        "- `{\"role\" : \"assistant\"}` : Assistant messages store previous assistant responses, but can also be written by you to give examples of desired behavior.\n",
        "\n",
        "The main idea is this:\n",
        "\n",
        "1. You start with a system message that outlines how the LLM should respond, what kind of behaviours you can expect from it, and more\n",
        "2. Then, you can provide a few examples in the form of \"assistant\"/\"user\" pairs\n",
        "3. Then, you prompt the model with the true \"user\" message.\n",
        "\n",
        "In this example, we'll be forgoing the 2nd step for simplicities sake."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdZ2KWKSR6yw"
      },
      "source": [
        "#### Utility Functions\n",
        "\n",
        "You'll notice that we're using some utility functions from the `aimakerspace` module - let's take a peek at these and see what they're doing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFbeJDDsR6yw"
      },
      "source": [
        "##### XYZRolePrompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mojJSE3R6yw"
      },
      "source": [
        "Here we have our `system`, `user`, and `assistant` role prompts.\n",
        "\n",
        "Let's take a peek at what they look like:\n",
        "\n",
        "```python\n",
        "class BasePrompt:\n",
        "    def __init__(self, prompt):\n",
        "        \"\"\"\n",
        "        Initializes the BasePrompt object with a prompt template.\n",
        "\n",
        "        :param prompt: A string that can contain placeholders within curly braces\n",
        "        \"\"\"\n",
        "        self.prompt = prompt\n",
        "        self._pattern = re.compile(r\"\\{([^}]+)\\}\")\n",
        "\n",
        "    def format_prompt(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Formats the prompt string using the keyword arguments provided.\n",
        "\n",
        "        :param kwargs: The values to substitute into the prompt string\n",
        "        :return: The formatted prompt string\n",
        "        \"\"\"\n",
        "        matches = self._pattern.findall(self.prompt)\n",
        "        return self.prompt.format(**{match: kwargs.get(match, \"\") for match in matches})\n",
        "\n",
        "    def get_input_variables(self):\n",
        "        \"\"\"\n",
        "        Gets the list of input variable names from the prompt string.\n",
        "\n",
        "        :return: List of input variable names\n",
        "        \"\"\"\n",
        "        return self._pattern.findall(self.prompt)\n",
        "```\n",
        "\n",
        "Then we have our `RolePrompt` which laser focuses us on the role pattern found in most API endpoints for LLMs.\n",
        "\n",
        "```python\n",
        "class RolePrompt(BasePrompt):\n",
        "    def __init__(self, prompt, role: str):\n",
        "        \"\"\"\n",
        "        Initializes the RolePrompt object with a prompt template and a role.\n",
        "\n",
        "        :param prompt: A string that can contain placeholders within curly braces\n",
        "        :param role: The role for the message ('system', 'user', or 'assistant')\n",
        "        \"\"\"\n",
        "        super().__init__(prompt)\n",
        "        self.role = role\n",
        "\n",
        "    def create_message(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Creates a message dictionary with a role and a formatted message.\n",
        "\n",
        "        :param kwargs: The values to substitute into the prompt string\n",
        "        :return: Dictionary containing the role and the formatted message\n",
        "        \"\"\"\n",
        "        return {\"role\": self.role, \"content\": self.format_prompt(**kwargs)}\n",
        "```\n",
        "\n",
        "We'll look at how the `SystemRolePrompt` is constructed to get a better idea of how that extension works:\n",
        "\n",
        "```python\n",
        "class SystemRolePrompt(RolePrompt):\n",
        "    def __init__(self, prompt: str):\n",
        "        super().__init__(prompt, \"system\")\n",
        "```\n",
        "\n",
        "That pattern is repeated for our `UserRolePrompt` and our `AssistantRolePrompt` as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D361R6sMR6yw"
      },
      "source": [
        "##### ChatOpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJVQ2Pm8R6yw"
      },
      "source": [
        "Next we have our model, which is converted to a format analagous to libraries like LangChain and LlamaIndex.\n",
        "\n",
        "Let's take a peek at how that is constructed:\n",
        "\n",
        "```python\n",
        "class ChatOpenAI:\n",
        "    def __init__(self, model_name: str = \"gpt-3.5-turbo\"):\n",
        "        self.model_name = model_name\n",
        "        self.openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "        if self.openai_api_key is None:\n",
        "            raise ValueError(\"OPENAI_API_KEY is not set\")\n",
        "\n",
        "    def run(self, messages, text_only: bool = True):\n",
        "        if not isinstance(messages, list):\n",
        "            raise ValueError(\"messages must be a list\")\n",
        "\n",
        "        openai.api_key = self.openai_api_key\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=self.model_name, messages=messages\n",
        "        )\n",
        "\n",
        "        if text_only:\n",
        "            return response.choices[0].message.content\n",
        "\n",
        "        return response\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCU7FfhIR6yw"
      },
      "source": [
        "#### ❓ Question #3:\n",
        "\n",
        "When calling the OpenAI API - are there any ways we can achieve more reproducible outputs?\n",
        "\n",
        "> NOTE: Check out [this section](https://platform.openai.com/docs/guides/text-generation/) of the OpenAI documentation for the answer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Answer #3:\n",
        "\n",
        "Yes, we can achieve more reproducible outputs by:\n",
        "1. Setting the `seed` parameter to a fixed value\n",
        "2. Setting the `temperature` parameter to a fixed smaller value, although lower values lead to a decrease in performance. See [this section for more](https://eugeneyan.com/writing/prompting/#selecting-a-temperature)\n",
        "3. Using `JSON` mode or function calling to get structured outputs that can be easily parsed and stored"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5wcjMLCR6yw"
      },
      "source": [
        "### Creating and Prompting OpenAI's `gpt-3.5-turbo`!\n",
        "\n",
        "Let's tie all these together and use it to prompt `gpt-3.5-turbo`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "WIfpIot7R6yw"
      },
      "outputs": [],
      "source": [
        "from aimakerspace.openai_utils.prompts import (\n",
        "    UserRolePrompt,\n",
        "    SystemRolePrompt,\n",
        "    AssistantRolePrompt,\n",
        ")\n",
        "\n",
        "from aimakerspace.openai_utils.chatmodel import ChatOpenAI\n",
        "\n",
        "chat_openai = ChatOpenAI()\n",
        "user_prompt_template = \"{content}\"\n",
        "user_role_prompt = UserRolePrompt(user_prompt_template)\n",
        "system_prompt_template = (\n",
        "    \"You are an expert in {expertise}, you always answer in a kind way.\"\n",
        ")\n",
        "system_role_prompt = SystemRolePrompt(system_prompt_template)\n",
        "\n",
        "messages = [\n",
        "    user_role_prompt.create_message(content=\"What is the best way to write a loop?\"),\n",
        "    system_role_prompt.create_message(expertise=\"Python\"),\n",
        "]\n",
        "\n",
        "response = chat_openai.run(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHo7lssNR6yw",
        "outputId": "1d3823fa-bb6b-45f6-ddba-b41686388324"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">There is no one <span style=\"color: #008000; text-decoration-color: #008000\">\"best\"</span> way to write a loop as it often depends on the specific requirements of the task at hand. \n",
              "However, in Python, a common and preferred way to write a loop is using a `for` loop when you know the number of \n",
              "iterations or an iterable to loop through, and a `while` loop when you need to loop based on a condition.\n",
              "\n",
              "Here is an example of a `for` loop in Python:\n",
              "\n",
              "```python\n",
              "for i in <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">range</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">)</span>:\n",
              "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">print</span><span style=\"font-weight: bold\">(</span>i<span style=\"font-weight: bold\">)</span>\n",
              "```\n",
              "\n",
              "And here is an example of a `while` loop in Python:\n",
              "\n",
              "```python\n",
              "count = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
              "while count &lt; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>:\n",
              "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">print</span><span style=\"font-weight: bold\">(</span>count<span style=\"font-weight: bold\">)</span>\n",
              "    count += <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
              "```\n",
              "\n",
              "Always remember to ensure that your loop has a clear exit condition to avoid infinite loops. Additionally, try to \n",
              "keep your loop logic simple and easy to understand to improve readability and maintainability of your code.\n",
              "</pre>\n"
            ],
            "text/plain": [
              "There is no one \u001b[32m\"best\"\u001b[0m way to write a loop as it often depends on the specific requirements of the task at hand. \n",
              "However, in Python, a common and preferred way to write a loop is using a `for` loop when you know the number of \n",
              "iterations or an iterable to loop through, and a `while` loop when you need to loop based on a condition.\n",
              "\n",
              "Here is an example of a `for` loop in Python:\n",
              "\n",
              "```python\n",
              "for i in \u001b[1;35mrange\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1m)\u001b[0m:\n",
              "    \u001b[1;35mprint\u001b[0m\u001b[1m(\u001b[0mi\u001b[1m)\u001b[0m\n",
              "```\n",
              "\n",
              "And here is an example of a `while` loop in Python:\n",
              "\n",
              "```python\n",
              "count = \u001b[1;36m0\u001b[0m\n",
              "while count < \u001b[1;36m5\u001b[0m:\n",
              "    \u001b[1;35mprint\u001b[0m\u001b[1m(\u001b[0mcount\u001b[1m)\u001b[0m\n",
              "    count += \u001b[1;36m1\u001b[0m\n",
              "```\n",
              "\n",
              "Always remember to ensure that your loop has a clear exit condition to avoid infinite loops. Additionally, try to \n",
              "keep your loop logic simple and easy to understand to improve readability and maintainability of your code.\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2nxxhB2R6yy"
      },
      "source": [
        "## Task 5: Retrieval Augmented Generation\n",
        "\n",
        "Now we can create a RAG prompt - which will help our system behave in a way that makes sense!\n",
        "\n",
        "There is much you could do here, many tweaks and improvements to be made!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "D1hamzGaR6yy"
      },
      "outputs": [],
      "source": [
        "RAG_PROMPT_TEMPLATE = \"\"\" \\\n",
        "Use the provided context to answer the user's query.\n",
        "\n",
        "You may not answer the user's query unless there is specific context in the following text.\n",
        "\n",
        "If you do not know the answer, or cannot answer, please respond with \"I don't know\".\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = SystemRolePrompt(RAG_PROMPT_TEMPLATE)\n",
        "\n",
        "USER_PROMPT_TEMPLATE = \"\"\" \\\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "User Query:\n",
        "{user_query}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "user_prompt = UserRolePrompt(USER_PROMPT_TEMPLATE)\n",
        "\n",
        "\n",
        "class RetrievalAugmentedQAPipeline:\n",
        "    def __init__(self, llm: ChatOpenAI(), vector_db_retriever: VectorDatabase) -> None:\n",
        "        self.llm = llm\n",
        "        self.vector_db_retriever = vector_db_retriever\n",
        "\n",
        "    def run_pipeline(self, user_query: str) -> str:\n",
        "        context_list = self.vector_db_retriever.search_by_text(user_query, k=4)\n",
        "\n",
        "        context_prompt = \"\"\n",
        "        for context in context_list:\n",
        "            context_prompt += context[0] + \"\\n\"\n",
        "\n",
        "        formatted_system_prompt = rag_prompt.create_message()\n",
        "\n",
        "        formatted_user_prompt = user_prompt.create_message(\n",
        "            user_query=user_query, context=context_prompt\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"response\": self.llm.run([formatted_user_prompt, formatted_system_prompt]),\n",
        "            \"context\": context_list,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZIJI19uR6yz"
      },
      "source": [
        "#### ❓ Question #4:\n",
        "\n",
        "What prompting strategies could you use to make the LLM have a more thoughtful, detailed response?\n",
        "\n",
        "What is that strategy called?\n",
        "\n",
        "> NOTE: You can look through the Week 1 Day 2 material for an answer to this question!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Answer #4:\n",
        "Chain-of-Thought strategy can be used to make the LLM have a more thoughtful, detailed response. This strategy involves providing the model with an additional instruction to reason and think about its actions, thereby activating certain parts of the model. Additionally, multiple other prompting strategies such as scratchpad, multi-turn, and more can be used to make the LLM be more logical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "kqbE9fZ6R6yz"
      },
      "outputs": [],
      "source": [
        "retrieval_augmented_qa_pipeline = RetrievalAugmentedQAPipeline(\n",
        "    vector_db_retriever=vector_db, llm=chat_openai\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAGhaCGOR6yz",
        "outputId": "e4fb3a1b-d2bc-4e18-ec31-dc0adf767163"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[32m'response'\u001b[0m: \u001b[32m'The \"Michael Eisner Memorial Weak Executive Problem\" refers to a situation where a CEO or startup founder hires a weak executive for a particular function because they have a hard time letting go of that function themselves. This often happens when the CEO or founder used to hold that position and wants to maintain control over it, resulting in the hiring of a weak executive who may not be able to fulfill the role effectively. This term is named after Michael Eisner, the former CEO of Disney, who faced a similar issue when he bought ABC at Disney and it fell to fourth place, indicating a lack of success in that area under his leadership.'\u001b[0m,\n",
              "    \u001b[32m'context'\u001b[0m: \u001b[1m[\u001b[0m\n",
              "        \u001b[1m(\u001b[0m\n",
              "            \u001b[32m'ordingly.\\nSeventh, when hiring the executive to run your former specialty, be\\ncareful you don’t hire someone weak on purpose.\\nThis sounds silly, but you wouldn’t believe how oaen it happens.\\nThe CEO who used to be a product manager who has a weak\\nproduct management executive. The CEO who used to be in\\nsales who has a weak sales executive. The CEO who used to be\\nin marketing who has a weak marketing executive.\\nI call this the “Michael Eisner Memorial Weak Executive Problem” — aaer the CEO of Disney who had previously been a brilliant TV network executive. When he bought ABC at Disney, it\\npromptly fell to fourth place. His response? “If I had an extra\\ntwo days a week, I could turn around ABC myself.” Well, guess\\nwhat, he didn’t have an extra two days a week.\\nA CEO — or a startup founder — oaen has a hard time letting\\ngo of the function that brought him to the party. The result: you\\nhire someone weak into the executive role for that function so\\nthat you can continue to be “the man” — cons'\u001b[0m,\n",
              "            \u001b[1;36m0.6581809295589605\u001b[0m\n",
              "        \u001b[1m)\u001b[0m,\n",
              "        \u001b[1m(\u001b[0m\n",
              "            \u001b[32m'm. They have areas where they are truly deXcient in judgment or skill set. That’s just life. Almost nobody is brilliant\\nat everything. When hiring and when Hring executives, you\\nmust therefore focus on strength rather than lack of weakness. Everybody has severe weaknesses even if you can’t see\\nthem yet. When managing, it’s oaen useful to micromanage and\\nto provide remedial training around these weaknesses. Doing so\\nmay make the diWerence between an executive succeeding or\\nfailing.\\nFor example, you might have a brilliant engineering executive\\nwho generates excellent team loyalty, has terriXc product judgment and makes the trains run on time. This same executive\\nmay be very poor at relating to the other functions in the company. She may generate far more than her share of cross-functional conYicts, cut herself oW from critical information, and\\nsigniXcantly impede your ability to sell and market eWectively.\\nYour alternatives are:\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32ma\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Macro-manage and give her an annual or quarterly object'\u001b[0m,\n",
              "            \u001b[1;36m0.5088024333618564\u001b[0m\n",
              "        \u001b[1m)\u001b[0m,\n",
              "        \u001b[1m(\u001b[0m\n",
              "            \u001b[32m'ed?\\nIn reality — as opposed to Marc’s warped view of reality — it will\\nbe extremely helpful for Marc \u001b[0m\u001b[32m[\u001b[0m\u001b[32mif he were actually the CEO,\\nwhich he is not\u001b[0m\u001b[32m]\u001b[0m\u001b[32m to meet with the new head of engineering daily\\nwhen she comes on board and review all of her thinking and\\ndecisions. This level of micromanagement will accelerate her\\ntraining and improve her long-term eWectiveness. It will make\\nher seem smarter to the rest of the organization which will build\\ncredibility and conXdence while she comes up to speed. Micromanaging new executives is generally a good idea for a limited\\nperiod of time.\\nHowever, that is not the only time that it makes sense to micro66 The Pmarca Blog Archives\\nmanage executives. It turns out that just about every executive\\nin the world has a few things that are seriously wrong with\\nthem. They have areas where they are truly deXcient in judgment or skill set. That’s just life. Almost nobody is brilliant\\nat everything. When hiring and when Hring executives, you\\nmust therefore focus o'\u001b[0m,\n",
              "            \u001b[1;36m0.4790009474592339\u001b[0m\n",
              "        \u001b[1m)\u001b[0m,\n",
              "        \u001b[1m(\u001b[0m\n",
              "            \u001b[32m'nYicts, cut herself oW from critical information, and\\nsigniXcantly impede your ability to sell and market eWectively.\\nYour alternatives are:\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32ma\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Macro-manage and give her an annual or quarterly objective\\nto Xx it, or…\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Intensively micromanage her interactions until she learns\\nthe fundamental interpersonal skills required to be an eWective\\nexecutive.\\nI am arguing that doing \u001b[0m\u001b[32m(\u001b[0m\u001b[32ma\u001b[0m\u001b[32m)\u001b[0m\u001b[32m will likely result in weak performance. The reason is that she very likely has no idea how to be\\neWective with her peers. If somebody is an executive, it’s very\\nlikely that somewhere along the line somebody gave her feedback — perhaps abstractly — about all of her weaknesses. Yet\\nthe weakness remains. As a result, executives generally require\\nmore hands-on management than lower level employees to\\nimprove weak areas.\\nSo, micromanagement is like Xne wine. A little at the right times\\nwill really enhance things; too much all the time and you’ll end\\nup in rehab.\\nPart 8: Hiring, managing, promoting, and Dring execut'\u001b[0m,\n",
              "            \u001b[1;36m0.46808123327748297\u001b[0m\n",
              "        \u001b[1m)\u001b[0m\n",
              "    \u001b[1m]\u001b[0m\n",
              "\u001b[1m}\u001b[0m"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_pipeline.run_pipeline(\n",
        "    \"What is the 'Michael Eisner Memorial Weak Executive Problem'?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🏗️ Activity #1:\n",
        "\n",
        "Enhance your RAG application in some way! \n",
        "\n",
        "Suggestions are: \n",
        "\n",
        "- Allow it to work with PDF files\n",
        "- Implement a new distance metric\n",
        "- Add metadata support to the vector database\n",
        "\n",
        "While these are suggestions, you should feel free to make whatever augmentations you desire! \n",
        "\n",
        "> NOTE: These additions might require you to work within the `aimakerspace` library - that's expected!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "from aimakerspace.pdf_utils import PDFLoader, CharacterPDFSplitter\n",
        "\n",
        "pdf_loader = PDFLoader(\"data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "documents = pdf_loader.load_documents()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "\u001b[1;36m186\u001b[0m"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pdf_splitter = CharacterPDFSplitter()\n",
        "split_documents_pdf = pdf_splitter.split_texts(documents)\n",
        "len(split_documents_pdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[32m'text'\u001b[0m: \u001b[32m'er model: the added layers are identity mapping, and the other layers are copied from the learned shallower model. The existence of this constructed solution indicates that a deeper model should produce no higher training error than its shallower counterpart. But experiments show that our current solvers on hand are unable to ﬁnd solutions that\\n\\n1\\n\\nidentity\\n\\nweight layer\\n\\nrelurelu\\n\\nF\u001b[0m\u001b[32m(\u001b[0m\u001b[32mx\u001b[0m\u001b[32m)\u001b[0m\u001b[32m(\u001b[0m\u001b[32mcid:1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m+\u001b[0m\u001b[32m(\u001b[0m\u001b[32mcid:1\u001b[0m\u001b[32m)\u001b[0m\u001b[32mxxF\u001b[0m\u001b[32m(\u001b[0m\u001b[32mx\u001b[0m\u001b[32m)\u001b[0m\u001b[32mx\\n\\nweight layer\\n\\nFigure 2. Residual learning: a building block.\\n\\nare comparably good or better than the constructed solution \u001b[0m\u001b[32m(\u001b[0m\u001b[32mor unable to do so in feasible time\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\nIn this paper, we address the degradation problem by introducing a deep residual In- stead of hoping each few stacked layers directly ﬁt a desired underlying mapping, we explicitly let these lay- ers ﬁt a residual mapping. Formally, denoting the desired \u001b[0m\u001b[32m(\u001b[0m\u001b[32mx\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, we let the stacked nonlinear underlying mapping as x. The orig- layers ﬁt another mapping of \u001b[0m\u001b[32m(\u001b[0m\u001b[32mx\u001b[0m\u001b[32m)\u001b[0m\u001b[32m F inal mapping is recast into \u001b[0m\u001b[32m(\u001b[0m\u001b[32mx\u001b[0m\u001b[32m)\u001b[0m\u001b[32m+x. We hypothesize that it is easier'\u001b[0m,\n",
              "    \u001b[32m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'filename'\u001b[0m: \u001b[32m'1512.03385v1.pdf'\u001b[0m, \u001b[32m'chunk_id'\u001b[0m: \u001b[1;36m5\u001b[0m\u001b[1m}\u001b[0m\n",
              "\u001b[1m}\u001b[0m"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_documents_pdf[5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "vector_db_pdf = VectorDatabase()\n",
        "vector_db_pdf = asyncio.run(vector_db_pdf.abuild_from_list(split_documents_pdf))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m[\u001b[0m\n",
              "    \u001b[1m(\u001b[0m\n",
              "        \u001b[32m'ct we counteract with Multi-Head Attention as described in section 3.2.\\n\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations \u001b[0m\u001b[32m[\u001b[0m\u001b[32m4, 27, 28, 22\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\n\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks \u001b[0m\u001b[32m[\u001b[0m\u001b[32m34\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\n\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention '\u001b[0m,\n",
              "        \u001b[1;36m0.5022802082386074\u001b[0m,\n",
              "        \u001b[1m{\u001b[0m\u001b[32m'filename'\u001b[0m: \u001b[32m'1706.03762v7.pdf'\u001b[0m, \u001b[32m'chunk_id'\u001b[0m: \u001b[1;36m7\u001b[0m\u001b[1m}\u001b[0m\n",
              "    \u001b[1m)\u001b[0m,\n",
              "    \u001b[1m(\u001b[0m\n",
              "        \u001b[32m'ion\\n\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n\\n3\\n\\nScaled Dot-Product Attention\\n\\nMulti-Head Attention\\n\\nFigure 2: \u001b[0m\u001b[32m(\u001b[0m\u001b[32mleft\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Scaled Dot-Product Attention. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mright\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Multi-Head Attention consists of several attention layers running in parallel.\\n\\nof the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\\n\\n3.2.1 Scaled Dot-Product Attention\\n\\nWe call our particular attention \"Scaled Dot-Product Attention\" \u001b[0m\u001b[32m(\u001b[0m\u001b[32mFigure 2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the dk, and apply a softmax function to obtain the weights on the query with all keys, divide each by values.\\n\\n√\\n\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys '\u001b[0m,\n",
              "        \u001b[1;36m0.43621382896979893\u001b[0m,\n",
              "        \u001b[1m{\u001b[0m\u001b[32m'filename'\u001b[0m: \u001b[32m'1706.03762v7.pdf'\u001b[0m, \u001b[32m'chunk_id'\u001b[0m: \u001b[1;36m11\u001b[0m\u001b[1m}\u001b[0m\n",
              "    \u001b[1m)\u001b[0m,\n",
              "    \u001b[1m(\u001b[0m\n",
              "        \u001b[32m' sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\\n\\nDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\\n\\n3.2 Attention\\n\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighte'\u001b[0m,\n",
              "        \u001b[1;36m0.4331328653502301\u001b[0m,\n",
              "        \u001b[1m{\u001b[0m\u001b[32m'filename'\u001b[0m: \u001b[32m'1706.03762v7.pdf'\u001b[0m, \u001b[32m'chunk_id'\u001b[0m: \u001b[1;36m10\u001b[0m\u001b[1m}\u001b[0m\n",
              "    \u001b[1m)\u001b[0m\n",
              "\u001b[1m]\u001b[0m"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_db_pdf.search_by_text(\"What is attention mechanism?\", k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "from aimakerspace.openai_utils.prompts import (\n",
        "    UserRolePrompt,\n",
        "    SystemRolePrompt,\n",
        "    AssistantRolePrompt,\n",
        ")\n",
        "\n",
        "from aimakerspace.openai_utils.chatmodel import ChatOpenAI\n",
        "\n",
        "RAG_PROMPT_TEMPLATE = \"\"\" \\\n",
        "Use the provided context to answer the user's query.\n",
        "\n",
        "You may not answer the user's query unless there is specific context in the following text.\n",
        "\n",
        "If you do not know the answer, or cannot answer, please respond with \"I don't know\".\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = SystemRolePrompt(RAG_PROMPT_TEMPLATE)\n",
        "\n",
        "USER_PROMPT_TEMPLATE = \"\"\" \\\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "User Query:\n",
        "{user_query}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "user_prompt = UserRolePrompt(USER_PROMPT_TEMPLATE)\n",
        "\n",
        "\n",
        "class RetrievalAugmentedQAPipeline:\n",
        "    def __init__(self, llm: ChatOpenAI(), vector_db_retriever: VectorDatabase) -> None:\n",
        "        self.llm = llm\n",
        "        self.vector_db_retriever = vector_db_retriever\n",
        "\n",
        "    def run_pipeline(self, user_query: str, k: int = 4) -> str:\n",
        "        context_list = self.vector_db_retriever.search_by_text(user_query, k=k)\n",
        "\n",
        "        context_prompt = \"\"\n",
        "        for context in context_list:\n",
        "            context_prompt += context[0] + \"\\n\"\n",
        "\n",
        "        formatted_system_prompt = rag_prompt.create_message()\n",
        "\n",
        "        formatted_user_prompt = user_prompt.create_message(\n",
        "            user_query=user_query, context=context_prompt\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"response\": self.llm.run([formatted_user_prompt, formatted_system_prompt]),\n",
        "            \"context\": context_list,\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[32m'response'\u001b[0m: \u001b[32m'An attention mechanism can be described as mapping a query and a set of key-value pairs to an output. The query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. There are different types of attention mechanisms, including additive attention and dot-product attention, with dot-product attention being faster and more space-efficient due to optimized matrix multiplication code.'\u001b[0m,\n",
              "    \u001b[32m'context'\u001b[0m: \u001b[1m[\u001b[0m\n",
              "        \u001b[1m(\u001b[0m\n",
              "            \u001b[32m'ct we counteract with Multi-Head Attention as described in section 3.2.\\n\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations \u001b[0m\u001b[32m[\u001b[0m\u001b[32m4, 27, 28, 22\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\n\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks \u001b[0m\u001b[32m[\u001b[0m\u001b[32m34\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\n\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention '\u001b[0m,\n",
              "            \u001b[1;36m0.5022802082386074\u001b[0m,\n",
              "            \u001b[1m{\u001b[0m\u001b[32m'filename'\u001b[0m: \u001b[32m'1706.03762v7.pdf'\u001b[0m, \u001b[32m'chunk_id'\u001b[0m: \u001b[1;36m7\u001b[0m\u001b[1m}\u001b[0m\n",
              "        \u001b[1m)\u001b[0m,\n",
              "        \u001b[1m(\u001b[0m\n",
              "            \u001b[32m'ion\\n\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n\\n3\\n\\nScaled Dot-Product Attention\\n\\nMulti-Head Attention\\n\\nFigure 2: \u001b[0m\u001b[32m(\u001b[0m\u001b[32mleft\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Scaled Dot-Product Attention. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mright\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Multi-Head Attention consists of several attention layers running in parallel.\\n\\nof the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\\n\\n3.2.1 Scaled Dot-Product Attention\\n\\nWe call our particular attention \"Scaled Dot-Product Attention\" \u001b[0m\u001b[32m(\u001b[0m\u001b[32mFigure 2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the dk, and apply a softmax function to obtain the weights on the query with all keys, divide each by values.\\n\\n√\\n\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys '\u001b[0m,\n",
              "            \u001b[1;36m0.43621382896979893\u001b[0m,\n",
              "            \u001b[1m{\u001b[0m\u001b[32m'filename'\u001b[0m: \u001b[32m'1706.03762v7.pdf'\u001b[0m, \u001b[32m'chunk_id'\u001b[0m: \u001b[1;36m11\u001b[0m\u001b[1m}\u001b[0m\n",
              "        \u001b[1m)\u001b[0m,\n",
              "        \u001b[1m(\u001b[0m\n",
              "            \u001b[32m' sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\\n\\nDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\\n\\n3.2 Attention\\n\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighte'\u001b[0m,\n",
              "            \u001b[1;36m0.4331328653502301\u001b[0m,\n",
              "            \u001b[1m{\u001b[0m\u001b[32m'filename'\u001b[0m: \u001b[32m'1706.03762v7.pdf'\u001b[0m, \u001b[32m'chunk_id'\u001b[0m: \u001b[1;36m10\u001b[0m\u001b[1m}\u001b[0m\n",
              "        \u001b[1m)\u001b[0m,\n",
              "        \u001b[1m(\u001b[0m\n",
              "            \u001b[32m' in case of the latter. The fundamental constraint of sequential computation, however, remains.\\n\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences \u001b[0m\u001b[32m[\u001b[0m\u001b[32m2, 19\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. In all but a few cases \u001b[0m\u001b[32m[\u001b[0m\u001b[32m27\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, however, such attention mechanisms are used in conjunction with a recurrent network.\\n\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n\\n2 Background\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU \u001b[0m\u001b[32m[\u001b[0m\u001b[32m16\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, ByteNet \u001b[0m\u001b[32m[\u001b[0m\u001b[32m18\u001b[0m\u001b[32m]\u001b[0m\u001b[32m and ConvS2S \u001b[0m\u001b[32m[\u001b[0m\u001b[32m9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, all of which use conv'\u001b[0m,\n",
              "            \u001b[1;36m0.43057056504269997\u001b[0m,\n",
              "            \u001b[1m{\u001b[0m\u001b[32m'filename'\u001b[0m: \u001b[32m'1706.03762v7.pdf'\u001b[0m, \u001b[32m'chunk_id'\u001b[0m: \u001b[1;36m5\u001b[0m\u001b[1m}\u001b[0m\n",
              "        \u001b[1m)\u001b[0m,\n",
              "        \u001b[1m(\u001b[0m\n",
              "            \u001b[32m'ameter matrices W Q and W O ∈ Rhdv×dmodel.\\n\\ni ∈ Rdmodel×dk , W K\\n\\ni ∈ Rdmodel×dk , W V\\n\\ni ∈ Rdmodel×dv\\n\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\\n\\n3.2.3 Applications of Attention in our Model\\n\\nThe Transformer uses multi-head attention in three different ways:\\n\\nIn \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as \u001b[0m\u001b[32m[\u001b[0m\u001b[32m38, 2, 9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\n\\nThe encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of'\u001b[0m,\n",
              "            \u001b[1;36m0.4235150042933461\u001b[0m,\n",
              "            \u001b[1m{\u001b[0m\u001b[32m'filename'\u001b[0m: \u001b[32m'1706.03762v7.pdf'\u001b[0m, \u001b[32m'chunk_id'\u001b[0m: \u001b[1;36m15\u001b[0m\u001b[1m}\u001b[0m\n",
              "        \u001b[1m)\u001b[0m,\n",
              "        \u001b[1m(\u001b[0m\n",
              "            \u001b[32m'it can be implemented using highly optimized matrix multiplication code.\\n\\n1√\\n\\ndk\\n\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk \u001b[0m\u001b[32m[\u001b[0m\u001b[32m3\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4. To counteract this effect, we scale the dot products by 1√ dk\\n\\n.\\n\\n3.2.2 Multi-Head Attention\\n\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n\\n4To illustrate why the dot products get large, assume that the components of q and k a'\u001b[0m,\n",
              "            \u001b[1;36m0.4174821091028509\u001b[0m,\n",
              "            \u001b[1m{\u001b[0m\u001b[32m'filename'\u001b[0m: \u001b[32m'1706.03762v7.pdf'\u001b[0m, \u001b[32m'chunk_id'\u001b[0m: \u001b[1;36m13\u001b[0m\u001b[1m}\u001b[0m\n",
              "        \u001b[1m)\u001b[0m,\n",
              "        \u001b[1m(\u001b[0m\n",
              "            \u001b[32m'st\\n\\n just\\n\\nnever\\n\\nits\\n\\nbe\\n\\nThe\\n\\n just\\n\\nshould\\n\\nbut\\n\\nwill\\n\\napplication\\n\\n just\\n\\n \u001b[0m\u001b[32m<\u001b[0m\u001b[32mpad\u001b[0m\u001b[32m>\\n\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5 and 6. Note that the attentions are very sharp for this word.\\n\\n14\\n\\nits\\n\\n just\\n\\nbe\\n\\n just\\n\\nbe\\n\\napplication\\n\\n,\\n\\nbut\\n\\njust\\n\\nperfect\\n\\nInput-Input Layer5\\n\\nnever\\n\\nThe\\n\\nshould\\n\\nwill\\n\\nLaw\\n\\n <pad\u001b[0m\u001b[32m>\u001b[0m\u001b[32m\\n\\n just\\n\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.\\n\\n15'\u001b[0m,\n",
              "            \u001b[1;36m0.40961778998537257\u001b[0m,\n",
              "            \u001b[1m{\u001b[0m\u001b[32m'filename'\u001b[0m: \u001b[32m'1706.03762v7.pdf'\u001b[0m, \u001b[32m'chunk_id'\u001b[0m: \u001b[1;36m48\u001b[0m\u001b[1m}\u001b[0m\n",
              "        \u001b[1m)\u001b[0m,\n",
              "        \u001b[1m(\u001b[0m\n",
              "            \u001b[32m'3 2 0 2\\n\\ng u A 2\\n\\n\u001b[0m\u001b[32m]\u001b[0m\u001b[32m L C . s c \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\n\\n7 v 2 6 7 3 0 . 6 0 7 1 : v i X r a\\n\\nProvided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works.\\n\\nAttention Is All You Need\\n\\nAshish Vaswani∗ Google Brain avaswani@google.com\\n\\nNoam Shazeer∗ Google Brain noam@google.com\\n\\nNiki Parmar∗ Google Research nikip@google.com\\n\\nJakob Uszkoreit∗ Google Research usz@google.com\\n\\nLlion Jones∗ Google Research llion@google.com\\n\\nAidan N. Gomez∗ † University of Toronto aidan@cs.toronto.edu\\n\\nŁukasz Kaiser∗ Google Brain lukaszkaiser@google.com\\n\\nIllia Polosukhin∗ ‡ illia.polosukhin@gmail.com\\n\\nAbstract\\n\\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely o'\u001b[0m,\n",
              "            \u001b[1;36m0.40027295292996107\u001b[0m,\n",
              "            \u001b[1m{\u001b[0m\u001b[32m'filename'\u001b[0m: \u001b[32m'1706.03762v7.pdf'\u001b[0m, \u001b[32m'chunk_id'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m\n",
              "        \u001b[1m)\u001b[0m,\n",
              "        \u001b[1m(\u001b[0m\n",
              "            \u001b[32m'specific tuning our model performs sur- prisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar \u001b[0m\u001b[32m[\u001b[0m\u001b[32m8\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\n\\nIn contrast to RNN sequence-to-sequence models \u001b[0m\u001b[32m[\u001b[0m\u001b[32m37\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, the Transformer outperforms the Berkeley- Parser \u001b[0m\u001b[32m[\u001b[0m\u001b[32m29\u001b[0m\u001b[32m]\u001b[0m\u001b[32m even when training only on the WSJ training set of 40K sentences.\\n\\n7 Conclusion\\n\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\\n\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\\n\\nWe are excited about the future of attention-based models'\u001b[0m,\n",
              "            \u001b[1;36m0.3988926101797832\u001b[0m,\n",
              "            \u001b[1m{\u001b[0m\u001b[32m'filename'\u001b[0m: \u001b[32m'1706.03762v7.pdf'\u001b[0m, \u001b[32m'chunk_id'\u001b[0m: \u001b[1;36m36\u001b[0m\u001b[1m}\u001b[0m\n",
              "        \u001b[1m)\u001b[0m,\n",
              "        \u001b[1m(\u001b[0m\n",
              "            \u001b[32m' obtain the weights on the query with all keys, divide each by values.\\n\\n√\\n\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as:\\n\\nAttention\u001b[0m\u001b[32m(\u001b[0m\u001b[32mQ, K, V \u001b[0m\u001b[32m)\u001b[0m\u001b[32m = softmax\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\n\\nQK T √ dk\\n\\n\u001b[0m\u001b[32m)\u001b[0m\u001b[32mV\\n\\nThe two most commonly used attention functions are additive attention \u001b[0m\u001b[32m[\u001b[0m\u001b[32m2\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, and dot-product \u001b[0m\u001b[32m(\u001b[0m\u001b[32mmulti- plicative\u001b[0m\u001b[32m)\u001b[0m\u001b[32m attention. Dot-product attention is identical to our algorithm, except for the scaling factor . Additive attention computes the compatibility function using a feed-forward network with of a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\\n\\n1√\\n\\ndk\\n\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attentio'\u001b[0m,\n",
              "            \u001b[1;36m0.39572224347073875\u001b[0m,\n",
              "            \u001b[1m{\u001b[0m\u001b[32m'filename'\u001b[0m: \u001b[32m'1706.03762v7.pdf'\u001b[0m, \u001b[32m'chunk_id'\u001b[0m: \u001b[1;36m12\u001b[0m\u001b[1m}\u001b[0m\n",
              "        \u001b[1m)\u001b[0m\n",
              "    \u001b[1m]\u001b[0m\n",
              "\u001b[1m}\u001b[0m"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_openai = ChatOpenAI()\n",
        "\n",
        "retrieval_augmented_qa_pipeline = RetrievalAugmentedQAPipeline(\n",
        "    vector_db_retriever=vector_db_pdf, llm=chat_openai\n",
        ")\n",
        "\n",
        "retrieval_augmented_qa_pipeline.run_pipeline(\"What is attention mechanism?\", k=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "out = retrieval_augmented_qa_pipeline.run_pipeline(\n",
        "    \"How well does ResNet perform on CIFAR-10 and CIFAR-100?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Response: ResNet performs well on the CIFAR-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> dataset, with training experiments showing successful results with \n",
              "extremely deep networks containing over <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span> layers. The optimization difficulties and the effectiveness of ResNet \n",
              "on CIFAR-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> are not specific to that dataset, as ResNet has also shown excellent results on the ImageNet \n",
              "classification dataset. Additionally, ResNet achieved <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.57</span>% top-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> error on the ImageNet test set and won first \n",
              "place in the ILSVRC <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2015</span> classification competition. Overall, ResNet has demonstrated strong performance on both \n",
              "CIFAR-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> and ImageNet datasets.\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Response: ResNet performs well on the CIFAR-\u001b[1;36m10\u001b[0m dataset, with training experiments showing successful results with \n",
              "extremely deep networks containing over \u001b[1;36m100\u001b[0m layers. The optimization difficulties and the effectiveness of ResNet \n",
              "on CIFAR-\u001b[1;36m10\u001b[0m are not specific to that dataset, as ResNet has also shown excellent results on the ImageNet \n",
              "classification dataset. Additionally, ResNet achieved \u001b[1;36m3.57\u001b[0m% top-\u001b[1;36m5\u001b[0m error on the ImageNet test set and won first \n",
              "place in the ILSVRC \u001b[1;36m2015\u001b[0m classification competition. Overall, ResNet has demonstrated strong performance on both \n",
              "CIFAR-\u001b[1;36m10\u001b[0m and ImageNet datasets.\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">RAG Outputs: \n",
              "</pre>\n"
            ],
            "text/plain": [
              "RAG Outputs: \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">====================\n",
              "</pre>\n"
            ],
            "text/plain": [
              "====================\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Context: e of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span> is slightly too large to start converging5. So we use <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.01</span> to warm up the training until th <span style=\"color: #808000; text-decoration-color: #808000\">...</span> \n",
              "nes denote testing error. Left: plain networks. The error of plain-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">110</span> is higher than <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span>% and not di\n",
              "Similarity: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.584423895417002</span>\n",
              "Reference: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'filename'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'1512.03385v1.pdf'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'chunk_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">42</span><span style=\"font-weight: bold\">}</span>\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Context: e of \u001b[1;36m0.1\u001b[0m is slightly too large to start converging5. So we use \u001b[1;36m0.01\u001b[0m to warm up the training until th \u001b[33m...\u001b[0m \n",
              "nes denote testing error. Left: plain networks. The error of plain-\u001b[1;36m110\u001b[0m is higher than \u001b[1;36m60\u001b[0m% and not di\n",
              "Similarity: \u001b[1;36m0.584423895417002\u001b[0m\n",
              "Reference: \u001b[1m{\u001b[0m\u001b[32m'filename'\u001b[0m: \u001b[32m'1512.03385v1.pdf'\u001b[0m, \u001b[32m'chunk_id'\u001b[0m: \u001b[1;36m42\u001b[0m\u001b[1m}\u001b[0m\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Context: lain-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">56</span>\n",
              "\n",
              "ResNet-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">110</span>\n",
              "\n",
              "Figure <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>. Training on CIFAR-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>. Dashed lines denote training error, and bold li <span style=\"color: #808000; text-decoration-color: #808000\">...</span>  on the PASCAL VOC <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2007</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2012</span> \n",
              "test sets using baseline Faster R-CNN. See also Ta- ble <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> and <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> for b\n",
              "Similarity: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5826403472390906</span>\n",
              "Reference: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'filename'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'1512.03385v1.pdf'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'chunk_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">43</span><span style=\"font-weight: bold\">}</span>\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Context: lain-\u001b[1;36m56\u001b[0m\n",
              "\n",
              "ResNet-\u001b[1;36m110\u001b[0m\n",
              "\n",
              "Figure \u001b[1;36m6\u001b[0m. Training on CIFAR-\u001b[1;36m10\u001b[0m. Dashed lines denote training error, and bold li \u001b[33m...\u001b[0m  on the PASCAL VOC \u001b[1;36m2007\u001b[0m/\u001b[1;36m2012\u001b[0m \n",
              "test sets using baseline Faster R-CNN. See also Ta- ble \u001b[1;36m10\u001b[0m and \u001b[1;36m11\u001b[0m for b\n",
              "Similarity: \u001b[1;36m0.5826403472390906\u001b[0m\n",
              "Reference: \u001b[1m{\u001b[0m\u001b[32m'filename'\u001b[0m: \u001b[32m'1512.03385v1.pdf'\u001b[0m, \u001b[32m'chunk_id'\u001b[0m: \u001b[1;36m43\u001b[0m\u001b[1m}\u001b[0m\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Context:  signiﬁcant accuracy gains from considerably increased depth. The beneﬁts of depth are witnessed for <span style=\"color: #808000; text-decoration-color: #808000\">...</span> \n",
              "training set and evaluated on the test set. Our focus is on the behaviors of extremely deep networks\n",
              "Similarity: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.577557127012256</span>\n",
              "Reference: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'filename'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'1512.03385v1.pdf'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'chunk_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">37</span><span style=\"font-weight: bold\">}</span>\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Context:  signiﬁcant accuracy gains from considerably increased depth. The beneﬁts of depth are witnessed for \u001b[33m...\u001b[0m \n",
              "training set and evaluated on the test set. Our focus is on the behaviors of extremely deep networks\n",
              "Similarity: \u001b[1;36m0.577557127012256\u001b[0m\n",
              "Reference: \u001b[1m{\u001b[0m\u001b[32m'filename'\u001b[0m: \u001b[32m'1512.03385v1.pdf'\u001b[0m, \u001b[32m'chunk_id'\u001b[0m: \u001b[1;36m37\u001b[0m\u001b[1m}\u001b[0m\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Context: CIFAR-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> set <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span><span style=\"font-weight: bold\">]</span>, suggesting that the optimization difﬁculties and the effects of our method are no <span style=\"color: #808000; text-decoration-color: #808000\">...</span> \n",
              "ns. This strong evidence shows that the residual learning principle is generic, and we expect that i\n",
              "Similarity: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5749994483298041</span>\n",
              "Reference: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'filename'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'1512.03385v1.pdf'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'chunk_id'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">}</span>\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Context: CIFAR-\u001b[1;36m10\u001b[0m set \u001b[1m[\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1m]\u001b[0m, suggesting that the optimization difﬁculties and the effects of our method are no \u001b[33m...\u001b[0m \n",
              "ns. This strong evidence shows that the residual learning principle is generic, and we expect that i\n",
              "Similarity: \u001b[1;36m0.5749994483298041\u001b[0m\n",
              "Reference: \u001b[1m{\u001b[0m\u001b[32m'filename'\u001b[0m: \u001b[32m'1512.03385v1.pdf'\u001b[0m, \u001b[32m'chunk_id'\u001b[0m: \u001b[1;36m8\u001b[0m\u001b[1m}\u001b[0m\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(f'Response: {out[\"response\"]}')\n",
        "\n",
        "print(\"RAG Outputs: \")\n",
        "print(\"==\" * 10)\n",
        "for context in out[\"context\"]:\n",
        "    print(\n",
        "        f\"Context: {context[0][:100]} ... {context[0][-100:]}\\n\"\n",
        "        f\"Similarity: {context[1]}\\n\"\n",
        "        f\"Reference: {context[2]}\\n\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QqpYXj2R6yz"
      },
      "source": [
        "## Task 6: Visibility Tooling\n",
        "\n",
        "This is great, but what if we wanted to add some visibility to our pipeline?\n",
        "\n",
        "Let's use Weights and Biases as a visibility tool!\n",
        "\n",
        "The first thing we'll need to do is create a Weights and Biases account and get an API key.\n",
        "\n",
        "You can follow the process outlined [here](https://docs.wandb.ai/quickstart) to do exactly that!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nST5OrAR6yz"
      },
      "source": [
        "Now we can get the Weights and Biases dependency and add our key to our env. to begin!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y54bofQQR6yz",
        "outputId": "6df38604-c2e0-4870-d5de-db165ea830ba"
      },
      "outputs": [],
      "source": [
        "!pip install -qU wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzunMw9_R6yz",
        "outputId": "f2506283-6b0d-4a1b-cb5a-65839c6445b1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "load_dotenv(find_dotenv())\n",
        "assert (\n",
        "    os.environ.get(\"WANDB_API_KEY\") is not None\n",
        "), \"Please set the WANDB_API_KEY environment variable.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "Iv1f27ebR6yz",
        "outputId": "34dd085e-0468-4657-d105-71ecc4b633fe"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "\u001b[1m<\u001b[0m\u001b[1;95mIPython.core.display.HTML\u001b[0m\u001b[39m object\u001b[0m\u001b[1m>\u001b[0m"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>e:\\Projects\\AI-Maven\\AIE3\\Week 2\\Day 1\\wandb\\run-20240614_164248-1ktcyy45</code>"
            ],
            "text/plain": [
              "\u001b[1m<\u001b[0m\u001b[1;95mIPython.core.display.HTML\u001b[0m\u001b[39m object\u001b[0m\u001b[1m>\u001b[0m"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lakshyaag/Visibility%20Example%20-%20AIE3/runs/1ktcyy45' target=\"_blank\">wild-butterfly-2</a></strong> to <a href='https://wandb.ai/lakshyaag/Visibility%20Example%20-%20AIE3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "\u001b[1m<\u001b[0m\u001b[1;95mIPython.core.display.HTML\u001b[0m\u001b[39m object\u001b[0m\u001b[1m>\u001b[0m"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/lakshyaag/Visibility%20Example%20-%20AIE3' target=\"_blank\">https://wandb.ai/lakshyaag/Visibility%20Example%20-%20AIE3</a>"
            ],
            "text/plain": [
              "\u001b[1m<\u001b[0m\u001b[1;95mIPython.core.display.HTML\u001b[0m\u001b[39m object\u001b[0m\u001b[1m>\u001b[0m"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/lakshyaag/Visibility%20Example%20-%20AIE3/runs/1ktcyy45' target=\"_blank\">https://wandb.ai/lakshyaag/Visibility%20Example%20-%20AIE3/runs/1ktcyy45</a>"
            ],
            "text/plain": [
              "\u001b[1m<\u001b[0m\u001b[1;95mIPython.core.display.HTML\u001b[0m\u001b[39m object\u001b[0m\u001b[1m>\u001b[0m"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/lakshyaag/Visibility%20Example%20-%20AIE3/runs/1ktcyy45?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "\u001b[1m<\u001b[0m\u001b[1;95mwandb.sdk.wandb_run.Run\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x00000248DE7AD050\u001b[0m\u001b[1m>\u001b[0m"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.init(project=\"Visibility Example - AIE3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iezOc48XR6yz"
      },
      "source": [
        "Now we can integrate Weights and Biases into our `RetrievalAugmentedQAPipeline`.\n",
        "\n",
        "```python\n",
        "if self.wandb_project:\n",
        "            root_span = Trace(\n",
        "                name=\"root_span\",\n",
        "                kind=\"llm\",\n",
        "                status_code=status,\n",
        "                status_message=status_message,\n",
        "                start_time_ms=start_time,\n",
        "                end_time_ms=end_time,\n",
        "                metadata={\n",
        "                    \"token_usage\" : token_usage\n",
        "                },\n",
        "                inputs= {\"system_prompt\" : formatted_system_prompt, \"user_prompt\" : formatted_user_prompt},\n",
        "                outputs= {\"response\" : response_text}\n",
        "            )\n",
        "\n",
        "            root_span.log(name=\"openai_trace\")\n",
        "```\n",
        "\n",
        "The main things to consider here are how to populate the various fields to make sure we're tracking useful information.\n",
        "\n",
        "We'll use the `text_only` flag to ensure we can get detailed information about our LLM call!\n",
        "\n",
        "You can check out all the parameters for Weights and Biases `Trace` [here](https://github.com/wandb/wandb/blob/653015a014281f45770aaf43627f64d9c4f04a32/wandb/sdk/data_types/trace_tree.py#L166)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "fqe4D27QR6yz"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "from wandb.sdk.data_types.trace_tree import Trace\n",
        "\n",
        "\n",
        "class RetrievalAugmentedGenerationPipeline:\n",
        "    def __init__(\n",
        "        self, llm: ChatOpenAI(), vector_db_retriever: VectorDatabase, wandb_project=None\n",
        "    ) -> None:\n",
        "        self.llm = llm\n",
        "        self.vector_db_retriever = vector_db_retriever\n",
        "        self.wandb_project = wandb_project\n",
        "\n",
        "    def run_pipeline(self, user_query: str, k: int = 4) -> str:\n",
        "        context_list = self.vector_db_retriever.search_by_text(user_query, k=k)\n",
        "\n",
        "        context_prompt = \"\"\n",
        "        for context in context_list:\n",
        "            context_prompt += context[0] + \"\\n\"\n",
        "\n",
        "        formatted_system_prompt = rag_prompt.create_message()\n",
        "\n",
        "        formatted_user_prompt = user_prompt.create_message(\n",
        "            user_query=user_query, context=context_prompt\n",
        "        )\n",
        "\n",
        "        start_time = datetime.datetime.now().timestamp() * 1000\n",
        "\n",
        "        try:\n",
        "            openai_response = self.llm.run(\n",
        "                [formatted_system_prompt, formatted_user_prompt], text_only=False\n",
        "            )\n",
        "            end_time = datetime.datetime.now().timestamp() * 1000\n",
        "            status = \"success\"\n",
        "            status_message = (None,)\n",
        "            response_text = openai_response.choices[0].message.content\n",
        "            token_usage = dict(openai_response.usage)\n",
        "            model = openai_response.model\n",
        "\n",
        "        except Exception as e:\n",
        "            end_time = datetime.datetime.now().timestamp() * 1000\n",
        "            status = \"error\"\n",
        "            status_message = str(e)\n",
        "            response_text = \"\"\n",
        "            token_usage = {}\n",
        "            model = \"\"\n",
        "\n",
        "        if self.wandb_project:\n",
        "            root_span = Trace(\n",
        "                name=\"root_span\",\n",
        "                kind=\"llm\",\n",
        "                status_code=status,\n",
        "                status_message=status_message,\n",
        "                start_time_ms=start_time,\n",
        "                end_time_ms=end_time,\n",
        "                metadata={\"token_usage\": token_usage, \"model_name\": model},\n",
        "                inputs={\n",
        "                    \"system_prompt\": formatted_system_prompt,\n",
        "                    \"user_prompt\": formatted_user_prompt,\n",
        "                },\n",
        "                outputs={\"response\": response_text},\n",
        "            )\n",
        "\n",
        "            root_span.log(name=\"openai_trace\")\n",
        "\n",
        "        return (\n",
        "            {\n",
        "                \"response\": self.llm.run(\n",
        "                    [formatted_user_prompt, formatted_system_prompt]\n",
        "                ),\n",
        "                \"context\": context_list,\n",
        "            }\n",
        "            if response_text\n",
        "            else \"We ran into an error. Please try again later. Full Error Message: \"\n",
        "            + status_message\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "aa1_0P1YR6yz"
      },
      "outputs": [],
      "source": [
        "retrieval_augmented_qa_pipeline = RetrievalAugmentedGenerationPipeline(\n",
        "    vector_db_retriever=vector_db_pdf,\n",
        "    llm=chat_openai,\n",
        "    wandb_project=\"LLM Visibility Example\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKSpyeg-R6yz",
        "outputId": "0d6358c9-4775-4734-f49d-851814de37c5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[32m'response'\u001b[0m: \u001b[32m\"I don't know.\"\u001b[0m,\n",
              "    \u001b[32m'context'\u001b[0m: \u001b[1m[\u001b[0m\n",
              "        \u001b[1m(\u001b[0m\n",
              "            \u001b[32m'-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as \u001b[0m\u001b[32m[\u001b[0m\u001b[32m17, 18\u001b[0m\u001b[32m]\u001b[0m\u001b[32m and \u001b[0m\u001b[32m[\u001b[0m\u001b[32m9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\n\\n3 Model Architecture\\n\\nMost competitive neural sequence transduction models have an encoder-decoder structure \u001b[0m\u001b[32m[\u001b[0m\u001b[32m5, 2, 35\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. Here, the encoder maps an input sequence of symbol representations \u001b[0m\u001b[32m(\u001b[0m\u001b[32mx1, ..., xn\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to a sequence of continuous representations z = \u001b[0m\u001b[32m(\u001b[0m\u001b[32mz1, ..., zn\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Given z, the decoder then generates an output sequence \u001b[0m\u001b[32m(\u001b[0m\u001b[32my1, ..., ym\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of symbols one element at a time. At each step the model is auto-regressive \u001b[0m\u001b[32m[\u001b[0m\u001b[32m10\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, consuming the previously generated symbols as additional input when generating the next.\\n\\n2\\n\\nFigure 1: The Transformer - model architecture.\\n\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and ri'\u001b[0m,\n",
              "            \u001b[1;36m0.11163700651666089\u001b[0m,\n",
              "            \u001b[1m{\u001b[0m\u001b[32m'filename'\u001b[0m: \u001b[32m'1706.03762v7.pdf'\u001b[0m, \u001b[32m'chunk_id'\u001b[0m: \u001b[1;36m8\u001b[0m\u001b[1m}\u001b[0m\n",
              "        \u001b[1m)\u001b[0m,\n",
              "        \u001b[1m(\u001b[0m\n",
              "            \u001b[32m'esentasimplebutoften-neglectedobservationthatthisphenomenonarisesnotjustduetooptimizationchallengesandinsteadevenifwecansolvetheoptimizationproblemsexactly,thelatentcodeshouldstillbeignoredatoptimumformostpracticalinstancesofVAEthathavein-tractabletrueposteriordistributionsandsufﬁcientlypowerfuldecoders.ItiseasiesttounderstandthisobservationfromaBits-BackCodingperspectiveofVAE.Itiswell-knownthatBits-BackCodingisaninformation-theoreticviewofVariationalInference\u001b[0m\u001b[32m(\u001b[0m\u001b[32mHinton&VanCamp,1993;Honkela&Valpola,2004\u001b[0m\u001b[32m)\u001b[0m\u001b[32mandspeciﬁclinkshavebeenestablishedbetweenBits-BackCodingandtheHelmholtzMachine/VAE\u001b[0m\u001b[32m(\u001b[0m\u001b[32mHinton&Zemel,1994;Gregoretal.,2013\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.HerewebrieﬂyrelateVAEtoBits-BackCodingforself-containedness:Firstrecallthatthegoalofdesigninganefﬁcientcodingprotocolistominimizetheexpectedcodelengthofcommunicatingx.ToexplainBits-BackCoding,let’sﬁrstconsideramorenaivecodingscheme.VAEcanbeseenasawaytoencodedatainatwo-partcode:p\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz\u001b[0m\u001b[32m)\u001b[0m\u001b[32mandp\u001b[0m\u001b[32m(\u001b[0m\u001b[32mx|z\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,wherezcanbeseenastheessence/structureofadatumandisencodedﬁrstandthenthemodelingerro'\u001b[0m,\n",
              "            \u001b[1;36m0.09150069138646152\u001b[0m,\n",
              "            \u001b[1m{\u001b[0m\u001b[32m'filename'\u001b[0m: \u001b[32m'1611.02731v2.pdf'\u001b[0m, \u001b[32m'chunk_id'\u001b[0m: \u001b[1;36m10\u001b[0m\u001b[1m}\u001b[0m\n",
              "        \u001b[1m)\u001b[0m,\n",
              "        \u001b[1m(\u001b[0m\n",
              "            \u001b[32m'wsetoflikelylocalstatisticsaregeneratedinthedecompressedimages:thebinarymasksareusuallydifferentandlocalstyleslikestrokewidtharesometimesslightlydif-ferent.However,weremarkthatthelossycodezdoesn’talwayscapturethekindofglobalinformationthatwecareaboutandit’sdependentonthetypeofconstraintweputonthedecoder.Forinstance,inFig4b,weshowdecompressionsforOMNIGLOTdataset,whichhasmoremeaningfulvariationsinsmallpatchesthanMNIST,andwecanobservethatsemanticsarenotpreservedinsomecases.Thishighlightstheneedtospecifythetypeofstatisticswecareaboutinarepresentation,whichwillbedifferentacrosstasksanddatasets,anddesigndecodingdistributionaccordingly.\\n\\nModelNLLTest\\n\\nPublishedasaconferencepaperatICLR2017\\n\\nTable4:Caltech-101Silhouettes.\u001b[0m\u001b[32m[\u001b[0m\u001b[32m1\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mBorn-schein&Bengio,2014\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\u001b[0m\u001b[32m[\u001b[0m\u001b[32m2\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mChoetal.,2011\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\u001b[0m\u001b[32m[\u001b[0m\u001b[32m3\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mDuetal.,2015\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\u001b[0m\u001b[32m[\u001b[0m\u001b[32m4\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mRolfe,2016\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\u001b[0m\u001b[32m[\u001b[0m\u001b[32m5\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32mGoessling&Amit,2015\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\n\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2016\u001b[0m\u001b[32m)\u001b[0m\u001b[32monstaticallybinarizedMNISTandmakethesinglemodiﬁcationofreplacingtheoriginalIAFposteriorwithanequivalentAFprior,removingthecontext.AsseeninTable1,VAEwithAFpriorisou'\u001b[0m,\n",
              "            \u001b[1;36m0.0868905465811941\u001b[0m,\n",
              "            \u001b[1m{\u001b[0m\u001b[32m'filename'\u001b[0m: \u001b[32m'1611.02731v2.pdf'\u001b[0m, \u001b[32m'chunk_id'\u001b[0m: \u001b[1;36m28\u001b[0m\u001b[1m}\u001b[0m\n",
              "        \u001b[1m)\u001b[0m,\n",
              "        \u001b[1m(\u001b[0m\n",
              "            \u001b[32m'llexistandwillnotbenegligible.OnceweunderstandtheinefﬁciencyoftheBits-BackCodingmechanism,it’ssimpletorealizewhysometimesthelatentcodezisnotused:ifthep\u001b[0m\u001b[32m(\u001b[0m\u001b[32mx|z\u001b[0m\u001b[32m)\u001b[0m\u001b[32mcouldmodelpdata\u001b[0m\u001b[32m(\u001b[0m\u001b[32mx\u001b[0m\u001b[32m)\u001b[0m\u001b[32mwithoutusinginforma-tionfromz,thenitwillnotusez,inwhichcasethetrueposteriorp\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32missimplythepriorp\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz\u001b[0m\u001b[32m)\u001b[0m\u001b[32mandit’susuallyeasytosetq\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32mtobep\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz\u001b[0m\u001b[32m)\u001b[0m\u001b[32mtoavoidincurringanextracostDKL\u001b[0m\u001b[32m(\u001b[0m\u001b[32mq\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m||p\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.Andit’sexactlythecasewhenapowerfuldecodingdistributionisusedlikeanRNNautoregressivedistribution,whichgivenenoughcapacityisabletomodelarbitrarilycomplexdistributions.HencethereexistsapreferenceofinformationwhenaVAEisoptimized:informationthatcanbemodeledlocallybydecodingdistributionp\u001b[0m\u001b[32m(\u001b[0m\u001b[32mx|z\u001b[0m\u001b[32m)\u001b[0m\u001b[32mwithoutaccesstozwillbeencodedlocallyandonlytheremainderwillbeencodedinz.Wenotethatonecommonwaytoencourageputtinginformationintothecodeistouseafactorizeddecoderp\u001b[0m\u001b[32m(\u001b[0m\u001b[32mx|z\u001b[0m\u001b[32m)\u001b[0m\u001b[32m=Qip\u001b[0m\u001b[32m(\u001b[0m\u001b[32mxi|z\u001b[0m\u001b[32m)\u001b[0m\u001b[32mbutsolongasthereisonedimensionxjthat’sindependentofallotherdimensionsfortruedatadistribution,pdata\u001b[0m\u001b[32m(\u001b[0m\u001b[32mx\u001b[0m\u001b[32m)\u001b[0m\u001b[32m=pdata\u001b[0m\u001b[32m(\u001b[0m\u001b[32mxj\u001b[0m\u001b[32m)\u001b[0m\u001b[32mpdata\u001b[0m\u001b[32m(\u001b[0m\u001b[32mx6\u001b[0m\u001b[32m=\u001b[0m\u001b[32mj\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,thenthelatentcodedoesn’tcontainalltheinformationaboutxsi'\u001b[0m,\n",
              "            \u001b[1;36m0.08126699994184645\u001b[0m,\n",
              "            \u001b[1m{\u001b[0m\u001b[32m'filename'\u001b[0m: \u001b[32m'1611.02731v2.pdf'\u001b[0m, \u001b[32m'chunk_id'\u001b[0m: \u001b[1;36m14\u001b[0m\u001b[1m}\u001b[0m\n",
              "        \u001b[1m)\u001b[0m\n",
              "    \u001b[1m]\u001b[0m\n",
              "\u001b[1m}\u001b[0m"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_pipeline.run_pipeline(\"Who is Batman?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbB0xcCRR6yz",
        "outputId": "7ba2fc10-abbe-40ec-eca8-22e1116300a5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[32m'response'\u001b[0m: \u001b[32m\"I don't know.\"\u001b[0m,\n",
              "    \u001b[32m'context'\u001b[0m: \u001b[1m[\u001b[0m\n",
              "        \u001b[1m(\u001b[0m\n",
              "            \u001b[32m'ecoderisweakened\u001b[0m\u001b[32m(\u001b[0m\u001b[32mBowmanetal.,2015;Serbanetal.,2016;Fraccaroetal.,2016\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.TheunderstandingofwhenVAEdoesautoencodewillbeanessentialbuildingpieceforVLAE.2.1TECHNICALBACKGROUNDLetxbeobservedvariables,zlatentvariablesandletp\u001b[0m\u001b[32m(\u001b[0m\u001b[32mx,z\u001b[0m\u001b[32m)\u001b[0m\u001b[32mbetheparametricmodeloftheirjointdistribution,calledthegenerativemodeldeﬁnedoverthevariables.\u001b[0m\u001b[32mGivenadatasetX\u001b[0m\u001b[32m=\u001b[0m\u001b[32m{\u001b[0m\u001b[32mx1,...,xN\u001b[0m\u001b[32m}\u001b[0m\u001b[32mwewishtoperformmaximumlikelihoodlearningofitsparameters:logp\u001b[0m\u001b[32m(\u001b[0m\u001b[32mX\u001b[0m\u001b[32m)\u001b[0m\u001b[32m=\u001b[0m\u001b[32mNXi\u001b[0m\u001b[32m=\u001b[0m\u001b[32m1logp\u001b[0m\u001b[32m(\u001b[0m\u001b[32mx\u001b[0m\u001b[32m(\u001b[0m\u001b[32mi\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\u001b[0m\u001b[32m(\u001b[0m\u001b[32m1\u001b[0m\u001b[32m)\u001b[0m\u001b[32mbutingeneralthismarginallikelihoodisintractabletocomputeordifferentiatedirectlyforﬂexiblegenerativemodelsthathavehigh-dimensionallatentvariablesandﬂexiblepriorsandlikelihoods.Asolutionistointroduceq\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,aparametricinferencemodeldeﬁnedoverthelatentvariables,andoptimizethevariationallowerboundonthemarginallog-likelihoodofeachobservationx:logp\u001b[0m\u001b[32m(\u001b[0m\u001b[32mx\u001b[0m\u001b[32m)\u001b[0m\u001b[32m≥Eq\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32mlogp\u001b[0m\u001b[32m(\u001b[0m\u001b[32mx,z\u001b[0m\u001b[32m)\u001b[0m\u001b[32m−logq\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m]\u001b[0m\u001b[32m=L\u001b[0m\u001b[32m(\u001b[0m\u001b[32mx;θ\u001b[0m\u001b[32m)\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2\u001b[0m\u001b[32m)\u001b[0m\u001b[32mwhereθindicatestheparametersofpandqmodels.TherearevariouswaystooptimizethelowerboundL\u001b[0m\u001b[32m(\u001b[0m\u001b[32mx;θ\u001b[0m\u001b[32m)\u001b[0m\u001b[32m;forcontinuouszitcanbedoneefﬁ-cientlythroughare-parameterizationofq\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m(\u001b[0m\u001b[32mKingma&Welling,2013;Rezendeet'\u001b[0m,\n",
              "            \u001b[1;36m0.14353325511605872\u001b[0m,\n",
              "            \u001b[1m{\u001b[0m\u001b[32m'filename'\u001b[0m: \u001b[32m'1611.02731v2.pdf'\u001b[0m, \u001b[32m'chunk_id'\u001b[0m: \u001b[1;36m6\u001b[0m\u001b[1m}\u001b[0m\n",
              "        \u001b[1m)\u001b[0m,\n",
              "        \u001b[1m(\u001b[0m\n",
              "            \u001b[32m'Equations \u001b[0m\u001b[32m(\u001b[0m\u001b[32mPDEs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, the widely used Multigrid method \u001b[0m\u001b[32m[\u001b[0m\u001b[32m3\u001b[0m\u001b[32m]\u001b[0m\u001b[32m reformulates the system as subprob- lems at multiple scales, where each subproblem is respon- sible for the residual solution between a coarser and a ﬁner scale. An alternative to Multigrid is hierarchical basis pre- conditioning \u001b[0m\u001b[32m[\u001b[0m\u001b[32m45, 46\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, which relies on variables that repre- sent residual vectors between two scales. It has been shown \u001b[0m\u001b[32m[\u001b[0m\u001b[32m3, 45, 46\u001b[0m\u001b[32m]\u001b[0m\u001b[32m that these solvers converge much faster than stan- dard solvers that are unaware of the residual nature of the solutions. These methods suggest that a good reformulation or preconditioning can simplify the optimization.\\n\\nShortcut Connections. Practices and theories that lead to shortcut connections \u001b[0m\u001b[32m[\u001b[0m\u001b[32m2, 34, 49\u001b[0m\u001b[32m]\u001b[0m\u001b[32m have been studied for a long time. An early practice of training multi-layer perceptrons \u001b[0m\u001b[32m(\u001b[0m\u001b[32mMLPs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m is to add a linear layer connected from the network input to the output \u001b[0m\u001b[32m[\u001b[0m\u001b[32m34, 49\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. In \u001b[0m\u001b[32m[\u001b[0m\u001b[32m44, 24\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, a few interme- diate layers are directly connected to auxiliary classiﬁers for addressing va'\u001b[0m,\n",
              "            \u001b[1;36m0.13995239897413464\u001b[0m,\n",
              "            \u001b[1m{\u001b[0m\u001b[32m'filename'\u001b[0m: \u001b[32m'1512.03385v1.pdf'\u001b[0m, \u001b[32m'chunk_id'\u001b[0m: \u001b[1;36m10\u001b[0m\u001b[1m}\u001b[0m\n",
              "        \u001b[1m)\u001b[0m,\n",
              "        \u001b[1m(\u001b[0m\n",
              "            \u001b[32m'thofVAE’stwo-partcode,weneedtosubtracttheextrainformationfromq\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.UsingBit-BackCoding,theexpectedcodelengthequatestothenegativevariationallowerboundortheso-calledHelmholtzvariationalfreeenergy,whichmeansminimizingcodelengthisequivalenttomaximizingthevariationallowerbound:CBitsBack\u001b[0m\u001b[32m(\u001b[0m\u001b[32mx\u001b[0m\u001b[32m)\u001b[0m\u001b[32m=Ex∼data,z∼q\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32mlogq\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m−logp\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz\u001b[0m\u001b[32m)\u001b[0m\u001b[32m−logp\u001b[0m\u001b[32m(\u001b[0m\u001b[32mx|z\u001b[0m\u001b[32m)\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32m6\u001b[0m\u001b[32m)\u001b[0m\u001b[32m=Ex∼data\u001b[0m\u001b[32m[\u001b[0m\u001b[32m−L\u001b[0m\u001b[32m(\u001b[0m\u001b[32mx\u001b[0m\u001b[32m)\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32m7\u001b[0m\u001b[32m)\u001b[0m\u001b[32mCastingtheproblemofoptimizingVAEintodesigninganefﬁcientcodingschemeeasilyallowsustoreasonwhenthelatentcodezwillbeused:thelatentcodezwillbeusedwhenthetwo-partcodeisanefﬁcientcode.Recallingthatthelower-boundofexpectedcodelengthfordataisgivenbytheShannonentropyofdatagenerationdistribution:H\u001b[0m\u001b[32m(\u001b[0m\u001b[32mdata\u001b[0m\u001b[32m)\u001b[0m\u001b[32m=Ex∼data\u001b[0m\u001b[32m[\u001b[0m\u001b[32m−logpdata\u001b[0m\u001b[32m(\u001b[0m\u001b[32mx\u001b[0m\u001b[32m)\u001b[0m\u001b[32m]\u001b[0m\u001b[32m,wecananalyzeVAE’scodingefﬁciency:CBitsBack\u001b[0m\u001b[32m(\u001b[0m\u001b[32mx\u001b[0m\u001b[32m)\u001b[0m\u001b[32m=Ex∼data,z∼q\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m[\u001b[0m\u001b[32mlogq\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m−logp\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz\u001b[0m\u001b[32m)\u001b[0m\u001b[32m−logp\u001b[0m\u001b[32m(\u001b[0m\u001b[32mx|z\u001b[0m\u001b[32m)\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32m8\u001b[0m\u001b[32m)\u001b[0m\u001b[32m=Ex∼data\u001b[0m\u001b[32m[\u001b[0m\u001b[32m−logp\u001b[0m\u001b[32m(\u001b[0m\u001b[32mx\u001b[0m\u001b[32m)\u001b[0m\u001b[32m+DKL\u001b[0m\u001b[32m(\u001b[0m\u001b[32mq\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m||p\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32m9\u001b[0m\u001b[32m)\u001b[0m\u001b[32m≥Ex∼data\u001b[0m\u001b[32m[\u001b[0m\u001b[32m−logpdata\u001b[0m\u001b[32m(\u001b[0m\u001b[32mx\u001b[0m\u001b[32m)\u001b[0m\u001b[32m+DKL\u001b[0m\u001b[32m(\u001b[0m\u001b[32mq\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m||p\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32m10\u001b[0m\u001b[32m)\u001b[0m\u001b[32m=H\u001b[0m\u001b[32m(\u001b[0m\u001b[32mdata\u001b[0m\u001b[32m)\u001b[0m\u001b[32m+Ex∼data\u001b[0m\u001b[32m[\u001b[0m\u001b[32mDKL\u001b[0m\u001b[32m(\u001b[0m\u001b[32mq\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m||p\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m]\u001b[0m\u001b[32m(\u001b[0m\u001b[32m11\u001b[0m\u001b[32m)\u001b[0m\u001b[32m3\\n\\nPublishedasaconferencepaperatICLR2017\\n\\nSinceKullbackLeiblerdivergenceisalwaysnon-negative,weknowthatusing'\u001b[0m,\n",
              "            \u001b[1;36m0.13994308312366102\u001b[0m,\n",
              "            \u001b[1m{\u001b[0m\u001b[32m'filename'\u001b[0m: \u001b[32m'1611.02731v2.pdf'\u001b[0m, \u001b[32m'chunk_id'\u001b[0m: \u001b[1;36m12\u001b[0m\u001b[1m}\u001b[0m\n",
              "        \u001b[1m)\u001b[0m,\n",
              "        \u001b[1m(\u001b[0m\n",
              "            \u001b[32m'iv preprint arXiv:1602.02410, 2016.\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m16\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\n\\nInformation Processing Systems, \u001b[0m\u001b[32m(\u001b[0m\u001b[32mNIPS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, 2016.\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m17\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\n\\non Learning Representations \u001b[0m\u001b[32m(\u001b[0m\u001b[32mICLR\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, 2016.\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m18\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017.\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m19\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\n\\nIn International Conference on Learning Representations, 2017.\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m20\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m21\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\n\\narXiv:1703.10722, 2017.\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m22\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Be'\u001b[0m,\n",
              "            \u001b[1;36m0.13033998249207346\u001b[0m,\n",
              "            \u001b[1m{\u001b[0m\u001b[32m'filename'\u001b[0m: \u001b[32m'1706.03762v7.pdf'\u001b[0m, \u001b[32m'chunk_id'\u001b[0m: \u001b[1;36m41\u001b[0m\u001b[1m}\u001b[0m\n",
              "        \u001b[1m)\u001b[0m\n",
              "    \u001b[1m]\u001b[0m\n",
              "\u001b[1m}\u001b[0m"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_pipeline.run_pipeline(\"What are some tips for being an effective CEO?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[32m'response'\u001b[0m: \u001b[32m'ResNet performs well on the CIFAR-10 dataset, as shown in the provided context. The 110-layer ResNet converges successfully on CIFAR-10, with fewer parameters than other deep networks. However, the text does not mention specific performance results on CIFAR-100.'\u001b[0m,\n",
              "    \u001b[32m'context'\u001b[0m: \u001b[1m[\u001b[0m\n",
              "        \u001b[1m(\u001b[0m\n",
              "            \u001b[32m'e of 0.1 is slightly too large to start converging5. So we use 0.01 to warm up the training until the training error is below 80% \u001b[0m\u001b[32m(\u001b[0m\u001b[32mabout 400 iterations\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, and then go back to 0.1 and con- tinue training. The rest of the learning schedule is as done previously. This 110-layer network converges well \u001b[0m\u001b[32m(\u001b[0m\u001b[32mFig. 6, middle\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. It has fewer parameters than other deep and thin\\n\\n5With an initial learning rate of 0.1, it starts converging \u001b[0m\u001b[32m(\u001b[0m\u001b[32m<90% error\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nafter several epochs, but still reaches similar accuracy.\\n\\n3\\n\\n20\\n\\n4\\n\\n5\\n\\n0\\n\\nplain-44\\n\\n1\\n\\nresidual-110\\n\\n5\\n\\nResNet-20\\n\\n2\\n\\n5\\n\\niter. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1e4\u001b[0m\u001b[32m)\u001b[0m\u001b[32merror \u001b[0m\u001b[32m(\u001b[0m\u001b[32m%\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nplain-20\\n\\n6\\n\\n5\\n\\n0\\n\\n6\\n\\n20\\n\\n4\\n\\nresidual-1202\\n\\n10\\n\\n10\\n\\n6\\n\\n56-layer20-layer110-layer20-layer\\n\\niter. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1e4\u001b[0m\u001b[32m)\u001b[0m\u001b[32merror \u001b[0m\u001b[32m(\u001b[0m\u001b[32m%\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n0\\n\\n5\\n\\n10\\n\\nResNet-32\\n\\n0\\n\\nplain-32\\n\\nResNet-56\\n\\n2\\n\\n3\\n\\n5\\n\\n0\\n\\niter. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1e4\u001b[0m\u001b[32m)\u001b[0m\u001b[32merror \u001b[0m\u001b[32m(\u001b[0m\u001b[32m%\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nResNet-44\\n\\n20\\n\\n4\\n\\n1\\n\\n1\\n\\nplain-56\\n\\nResNet-110\\n\\nFigure 6. Training on CIFAR-10. Dashed lines denote training error, and bold lines denote testing error. Left: plain networks. The error of plain-110 is higher than 60% and not di'\u001b[0m,\n",
              "            \u001b[1;36m0.584423895417002\u001b[0m,\n",
              "            \u001b[1m{\u001b[0m\u001b[32m'filename'\u001b[0m: \u001b[32m'1512.03385v1.pdf'\u001b[0m, \u001b[32m'chunk_id'\u001b[0m: \u001b[1;36m42\u001b[0m\u001b[1m}\u001b[0m\n",
              "        \u001b[1m)\u001b[0m,\n",
              "        \u001b[1m(\u001b[0m\n",
              "            \u001b[32m'lain-56\\n\\nResNet-110\\n\\nFigure 6. Training on CIFAR-10. Dashed lines denote training error, and bold lines denote testing error. Left: plain networks. The error of plain-110 is higher than 60% and not displayed. Middle: ResNets. Right: ResNets with 110 and 1202 layers.\\n\\nplain-56\\n\\n2\\n\\n60\\n\\nplain-56\\n\\nResNet-110\\n\\n60\\n\\n100\\n\\nplain-20\\n\\n1\\n\\n2\\n\\n40\\n\\n3\\n\\nResNet-56\\n\\n20\\n\\n20\\n\\n100\\n\\nResNet-110\\n\\nplain-20\\n\\nResNet-20\\n\\n0\\n\\nResNet-56\\n\\n0\\n\\n80\\n\\n3\\n\\nlayer index \u001b[0m\u001b[32m(\u001b[0m\u001b[32msorted by magnitude\u001b[0m\u001b[32m)\u001b[0m\u001b[32mstd\\n\\nlayer index \u001b[0m\u001b[32m(\u001b[0m\u001b[32moriginal\u001b[0m\u001b[32m)\u001b[0m\u001b[32mstd\\n\\n1\\n\\n80\\n\\n40\\n\\nResNet-20\\n\\nFigure 7. Standard deviations \u001b[0m\u001b[32m(\u001b[0m\u001b[32mstd\u001b[0m\u001b[32m)\u001b[0m\u001b[32m of layer responses on CIFAR- 10. The responses are the outputs of each 3×3 layer, after BN and before nonlinearity. Top: the layers are shown in their original order. Bottom: the responses are ranked in descending order.\\n\\ntraining data test data VGG-16 ResNet-101\\n\\n07+12 VOC 07 test 73.2 76.4\\n\\n07++12 VOC 12 test 70.4 73.8\\n\\nTable 7. Object detection mAP \u001b[0m\u001b[32m(\u001b[0m\u001b[32m%\u001b[0m\u001b[32m)\u001b[0m\u001b[32m on the PASCAL VOC 2007/2012 test sets using baseline Faster R-CNN. See also Ta- ble 10 and 11 for b'\u001b[0m,\n",
              "            \u001b[1;36m0.5826403472390906\u001b[0m,\n",
              "            \u001b[1m{\u001b[0m\u001b[32m'filename'\u001b[0m: \u001b[32m'1512.03385v1.pdf'\u001b[0m, \u001b[32m'chunk_id'\u001b[0m: \u001b[1;36m43\u001b[0m\u001b[1m}\u001b[0m\n",
              "        \u001b[1m)\u001b[0m,\n",
              "        \u001b[1m(\u001b[0m\n",
              "            \u001b[32m' signiﬁcant accuracy gains from considerably increased depth. The beneﬁts of depth are witnessed for all evaluation metrics \u001b[0m\u001b[32m(\u001b[0m\u001b[32mTable 3 and 4\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\nComparisons with State-of-the-art Methods. In Table 4 we compare with the previous best single-model results. Our baseline 34-layer ResNets have achieved very compet- itive accuracy. Our 152-layer ResNet has a single-model top-5 validation error of 4.49%. This single-model result outperforms all previous ensemble results \u001b[0m\u001b[32m(\u001b[0m\u001b[32mTable 5\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. We combine six models of different depth to form an ensemble \u001b[0m\u001b[32m(\u001b[0m\u001b[32monly with two 152-layer ones at the time of submitting\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. This leads to 3.57% top-5 error on the test set \u001b[0m\u001b[32m(\u001b[0m\u001b[32mTable 5\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. This entry won the 1st place in ILSVRC 2015.\\n\\n4.2. CIFAR-10 and Analysis\\n\\nWe conducted more studies on the CIFAR-10 dataset \u001b[0m\u001b[32m[\u001b[0m\u001b[32m20\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, which consists of 50k training images and 10k test- ing images in 10 classes. We present experiments trained on the training set and evaluated on the test set. Our focus is on the behaviors of extremely deep networks'\u001b[0m,\n",
              "            \u001b[1;36m0.577557127012256\u001b[0m,\n",
              "            \u001b[1m{\u001b[0m\u001b[32m'filename'\u001b[0m: \u001b[32m'1512.03385v1.pdf'\u001b[0m, \u001b[32m'chunk_id'\u001b[0m: \u001b[1;36m37\u001b[0m\u001b[1m}\u001b[0m\n",
              "        \u001b[1m)\u001b[0m,\n",
              "        \u001b[1m(\u001b[0m\n",
              "            \u001b[32m'CIFAR-10 set \u001b[0m\u001b[32m[\u001b[0m\u001b[32m20\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, suggesting that the optimization difﬁculties and the effects of our method are not just akin to a particular dataset. We present successfully trained models on this dataset with over 100 layers, and explore models with over 1000 layers. On the ImageNet classiﬁcation dataset \u001b[0m\u001b[32m[\u001b[0m\u001b[32m36\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, we obtain excellent results by extremely deep residual nets. Our 152- layer residual net is the deepest network ever presented on ImageNet, while still having lower complexity than VGG nets \u001b[0m\u001b[32m[\u001b[0m\u001b[32m41\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. Our ensemble has 3.57% top-5 error on the\\n\\n2\\n\\nImageNet test set, and won the 1st place in the ILSVRC 2015 classiﬁcation competition. The extremely deep rep- resentations also have excellent generalization performance on other recognition tasks, and lead us to further win the 1st places on: ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation in ILSVRC & COCO 2015 competitions. This strong evidence shows that the residual learning principle is generic, and we expect that i'\u001b[0m,\n",
              "            \u001b[1;36m0.5749994483298041\u001b[0m,\n",
              "            \u001b[1m{\u001b[0m\u001b[32m'filename'\u001b[0m: \u001b[32m'1512.03385v1.pdf'\u001b[0m, \u001b[32m'chunk_id'\u001b[0m: \u001b[1;36m8\u001b[0m\u001b[1m}\u001b[0m\n",
              "        \u001b[1m)\u001b[0m\n",
              "    \u001b[1m]\u001b[0m\n",
              "\u001b[1m}\u001b[0m"
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_pipeline.run_pipeline(\n",
        "    \"How well does ResNet perform on CIFAR-10 and CIFAR-100?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAkIW2qRR6yz"
      },
      "source": [
        "Navigate to the Weights and Biases \"run\" link to see how your LLM is performing!\n",
        "\n",
        "> View run at [this](https://wandb.ai/lakshyaag/Visibility%20Example%20-%20AIE3/runs/1ktcyy45?nw=nwuserlakshyaag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sInZ32BoR6yz"
      },
      "source": [
        "#### ❓ Question #5:\n",
        "\n",
        "What is the `model_name` from the WandB `root_span` trace?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Answer #5:\n",
        "The `model_name` from the WandB `root_span` trace is `gpt-3.5-turbo-0125`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of8wJGalR6yz"
      },
      "source": [
        "## Task 7: RAG Evaluation Using GPT-4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "PRNzzMurR6yz",
        "outputId": "0dcaf542-530c-480d-9dc6-a2931e8557e2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ResNet performs well on the CIFAR-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> dataset. The text mentions that they have successfully trained models on \n",
              "CIFAR-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> with over <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span> layers and have explored models with over <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000</span> layers. Additionally, the text states that \n",
              "ResNet achieves excellent results on the ImageNet classification dataset, winning the 1st place in the ILSVRC <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2015</span> \n",
              "competition. However, there is no specific mention of ResNet's performance on the CIFAR-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span> dataset in the provided\n",
              "context.\n",
              "</pre>\n"
            ],
            "text/plain": [
              "ResNet performs well on the CIFAR-\u001b[1;36m10\u001b[0m dataset. The text mentions that they have successfully trained models on \n",
              "CIFAR-\u001b[1;36m10\u001b[0m with over \u001b[1;36m100\u001b[0m layers and have explored models with over \u001b[1;36m1000\u001b[0m layers. Additionally, the text states that \n",
              "ResNet achieves excellent results on the ImageNet classification dataset, winning the 1st place in the ILSVRC \u001b[1;36m2015\u001b[0m \n",
              "competition. However, there is no specific mention of ResNet's performance on the CIFAR-\u001b[1;36m100\u001b[0m dataset in the provided\n",
              "context.\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"clarity\": \"8\", \"faithfulness\": \"5\", \"correctness\": \"6\", \"detail\": \"4\"\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"How well does ResNet perform on CIFAR-10 and CIFAR-100?\"\n",
        "\n",
        "response = retrieval_augmented_qa_pipeline.run_pipeline(query)\n",
        "\n",
        "print(response[\"response\"])\n",
        "\n",
        "evaluator_system_template = \"\"\"You are an expert in analyzing the quality of a response.\n",
        "\n",
        "You should be hyper-critical.\n",
        "\n",
        "Provide scores (out of 10) for the following attributes:\n",
        "\n",
        "1. Clarity - how clear is the response\n",
        "2. Faithfulness - how related to the original query is the response and the provided context\n",
        "3. Correctness - was the response correct?\n",
        "4. Detail - how detailed was the response?\n",
        "\n",
        "Please take your time, and think through each item step-by-step, when you are done - please provide your response in the following JSON format:\n",
        "\n",
        "{\"clarity\" : \"score_out_of_10\", \"faithfulness\" : \"score_out_of_10\", \"correctness\" : \"score_out_of_10\"}, \"detail\" : \"score_out_of_10\"}\"\"\"\n",
        "\n",
        "evaluation_template = \"\"\"Query: {input}\n",
        "Context: {context}\n",
        "Response: {response}\"\"\"\n",
        "\n",
        "try:\n",
        "    chat_openai = ChatOpenAI(model_name=\"gpt-4-turbo\")\n",
        "\n",
        "except:\n",
        "    chat_openai = ChatOpenAI()\n",
        "\n",
        "evaluator_system_prompt = SystemRolePrompt(evaluator_system_template)\n",
        "evaluation_prompt = UserRolePrompt(evaluation_template)\n",
        "\n",
        "messages = [\n",
        "    evaluator_system_prompt.create_message(format=False),\n",
        "    evaluation_prompt.create_message(\n",
        "        input=query,\n",
        "        context=\"\\n\".join([context[0] for context in response[\"context\"]]),\n",
        "        response=response[\"response\"],\n",
        "    ),\n",
        "]\n",
        "\n",
        "chat_openai.run(messages, response_format={\"type\": \"json_object\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNpauQmJR6yz"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "In this notebook, we've gone through the steps required to create your own simple RAQA application!\n",
        "\n",
        "Please feel free to extend this as much as you'd like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "bb904e05ece143c79ecc4f20de482f45",
            "3a4ba348cb004f8ab7b2b1395539c81b",
            "1ce393d9afcf427d9d352259c5d32678",
            "56a8e24025594e5e9ff3b8581c344691",
            "d2ea5009dd16442cb5d8a0ac468e50a8",
            "5f00135fe1044051a50ee5e841cbb8e3",
            "4e6efd99f7d346e485b002fb0fa85cc7",
            "3dfb67c39958461da6071e4c19c3fa41"
          ]
        },
        "id": "xzlxJbFtR6y0",
        "outputId": "d5789d16-c41c-4a3c-ac53-65640a0d3698"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">wild-butterfly-2</strong> at: <a href='https://wandb.ai/lakshyaag/Visibility%20Example%20-%20AIE3/runs/1ktcyy45' target=\"_blank\">https://wandb.ai/lakshyaag/Visibility%20Example%20-%20AIE3/runs/1ktcyy45</a><br/> View project at: <a href='https://wandb.ai/lakshyaag/Visibility%20Example%20-%20AIE3' target=\"_blank\">https://wandb.ai/lakshyaag/Visibility%20Example%20-%20AIE3</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "\u001b[1m<\u001b[0m\u001b[1;95mIPython.core.display.HTML\u001b[0m\u001b[39m object\u001b[0m\u001b[1m>\u001b[0m"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20240614_164248-1ktcyy45\\logs</code>"
            ],
            "text/plain": [
              "\u001b[1m<\u001b[0m\u001b[1;95mIPython.core.display.HTML\u001b[0m\u001b[39m object\u001b[0m\u001b[1m>\u001b[0m"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "buildyourownlangchain",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1ce393d9afcf427d9d352259c5d32678": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e6efd99f7d346e485b002fb0fa85cc7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3dfb67c39958461da6071e4c19c3fa41",
            "value": 1
          }
        },
        "3a4ba348cb004f8ab7b2b1395539c81b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2ea5009dd16442cb5d8a0ac468e50a8",
            "placeholder": "​",
            "style": "IPY_MODEL_5f00135fe1044051a50ee5e841cbb8e3",
            "value": "0.018 MB of 0.018 MB uploaded\r"
          }
        },
        "3dfb67c39958461da6071e4c19c3fa41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e6efd99f7d346e485b002fb0fa85cc7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56a8e24025594e5e9ff3b8581c344691": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f00135fe1044051a50ee5e841cbb8e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb904e05ece143c79ecc4f20de482f45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a4ba348cb004f8ab7b2b1395539c81b",
              "IPY_MODEL_1ce393d9afcf427d9d352259c5d32678"
            ],
            "layout": "IPY_MODEL_56a8e24025594e5e9ff3b8581c344691"
          }
        },
        "d2ea5009dd16442cb5d8a0ac468e50a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
