{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgDepNVhvzIr"
      },
      "source": [
        "# OpenAI Assistants - Building Agentic RAG with the Function Calling, Retrieval, and Code Interpreter Tools\n",
        "\n",
        "Today we'll explore using OpenAI's Python SDK to create, manage, and use the OpenAI Assistant API!\n",
        "\n",
        "We'll be doing the following in today's notebook:\n",
        "\n",
        "1. Task 1: Simple Assistant\n",
        "2. Task 2: Adding Tools\n",
        "  - Task 2a: Creating an Assistant with File Search Tool\n",
        "  - Task 2b: Creating an Assistant with Code Interpreter Tool\n",
        "  - Task 2c: Creating an Assistant with Function Calling Tool\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Colab Specific Instructions:\n",
        "\n",
        "To get started, please make a copy of this notebook using `File > Save a copy in Drive`\n",
        "\n",
        "![image](https://i.imgur.com/rNzMEfs.png)\n",
        "\n",
        "You will be expected to submit a GitHub link to the completed notebook, so if you're completing the assignment on Colab - you'll want to download the notebook when you have completed it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNU6b3ymwOWq"
      },
      "source": [
        "## Dependencies\n",
        "\n",
        "We'll start, as we usually do, with some dependiencies and our API key!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from rich import print\n",
        "\n",
        "%load_ext rich"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePayyL6at6LS",
        "outputId": "4fb742b3-c650-44c2-8c58-2d83c81a766b"
      },
      "outputs": [],
      "source": [
        "!pip install -qU openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unKr3HZdu-1V",
        "outputId": "9c567820-9633-44fd-dad6-e7dd1ca46d97"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "load_dotenv(find_dotenv())\n",
        "\n",
        "assert os.getenv(\"OPENAI_API_KEY\") is not None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwNE7N4HwhXH"
      },
      "source": [
        "## Task 1: Simple Assistant\n",
        "\n",
        "Let's create a simple Assistant to understand more about how the API works to start!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKEwbLMFxNKs"
      },
      "source": [
        "### OpenAI Client\n",
        "\n",
        "At the core of the OpenAI Python SDK is the Client!\n",
        "\n",
        "> NOTE: For ease of use, we'll start with the synchronous `OpenAI()`. OpenAI does provide an `AsyncOpenAI()` that you could leverage as well!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wF-mBZwtuavl"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aIx4GZ2w_1c"
      },
      "source": [
        "### Creating An Assistant\n",
        "\n",
        "Leveraging what we know about the OpenAI API from previous sessions - we're going to start by simply initializing an Assistant.\n",
        "\n",
        "Before we begin, we need to think about a few customization options we have:\n",
        "\n",
        "- `name` - Straight forward enough, this is what our Assistant's name will be\n",
        "- `instructions` - similar to a system message, but applied at an Assistant level, this is how we can guide the Assistant's tone, behaviour, functionality, and more!\n",
        "- `model` - this will allow us to choose which model we would prefer to use for our Assistant\n",
        "\n",
        "Let's start by setting some instructions for our Assistant.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "cellView": "form",
        "id": "Paqd6zWMyMAJ"
      },
      "outputs": [],
      "source": [
        "# @markdown #### üèóÔ∏è Build Activity üèóÔ∏è\n",
        "# @markdown Fill out the fields below to add your Assistant's name, instructions, and desired model!\n",
        "\n",
        "name = \"General Chatbot\"  # @param {type: \"string\"}\n",
        "instructions = \"Answer in a professional manner.\"  # @param {type: \"string\"}\n",
        "model = \"gpt-4o\"  # @param [\"gpt-3.5-turbo\", \"gpt-4-turbo-preview\", \"gpt-4\", \"gpt-4o\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUeaDsLMzcv-"
      },
      "source": [
        "### Initialize Assistant\n",
        "\n",
        "Now that we have our desired name, instruction, and model - we can initialize our Assistant!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "6-4MgVLbu8rO"
      },
      "outputs": [],
      "source": [
        "assistant = client.beta.assistants.create(\n",
        "    name=name,\n",
        "    instructions=instructions,\n",
        "    model=model,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QskO5n5W2X6t"
      },
      "source": [
        "Let's examine our `assistant` object and see what we find!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkkIC_JP2bG0",
        "outputId": "86ecb682-a04d-4687-94d3-18a4b99ffc07"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1;35mAssistant\u001b[0m\u001b[1m(\u001b[0m\n",
              "    \u001b[33mid\u001b[0m=\u001b[32m'asst_aSPvLTXMt0Z1bfUAVZcNTcO6'\u001b[0m,\n",
              "    \u001b[33mcreated_at\u001b[0m=\u001b[1;36m1718401905\u001b[0m,\n",
              "    \u001b[33mdescription\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[33minstructions\u001b[0m=\u001b[32m'Answer in a professional manner.'\u001b[0m,\n",
              "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
              "    \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o'\u001b[0m,\n",
              "    \u001b[33mname\u001b[0m=\u001b[32m'General Chatbot'\u001b[0m,\n",
              "    \u001b[33mobject\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
              "    \u001b[33mtools\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
              "    \u001b[33mresponse_format\u001b[0m=\u001b[32m'auto'\u001b[0m,\n",
              "    \u001b[33mtemperature\u001b[0m=\u001b[1;36m1\u001b[0m\u001b[1;36m.0\u001b[0m,\n",
              "    \u001b[33mtool_resources\u001b[0m=\u001b[1;35mToolResources\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcode_interpreter\u001b[0m=\u001b[3;35mNone\u001b[0m, \u001b[33mfile_search\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m,\n",
              "    \u001b[33mtop_p\u001b[0m=\u001b[1;36m1\u001b[0m\u001b[1;36m.0\u001b[0m\n",
              "\u001b[1m)\u001b[0m"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "assistant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "615NK1Qj2e_Z"
      },
      "source": [
        "There are a number of useful parameters here, but we'll call out a few:\n",
        "\n",
        "- `id` - since we may have multiple Assistant's, knowing which Assistant we're interacting with will help us ensure the desired user experience!\n",
        "- `description` - A natrual language description of our Assistant could help others understand what it's supposed to do!\n",
        "- `file_ids` - if we wanted to use the Retrieval tool, this would let us know what files we had given our Assistant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3HhlqtM0AhW"
      },
      "source": [
        "### Creating a Thread\n",
        "\n",
        "Behind the scenes our Assistant is powered by the idea of \"threads\".\n",
        "\n",
        "You can think of threads as individual conversations that interact with the Assistant.\n",
        "\n",
        "Let's create a thread now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "iFVM39vevT5f"
      },
      "outputs": [],
      "source": [
        "thread = client.beta.threads.create()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y7jelq01PoG"
      },
      "source": [
        "Let's look at our `thread` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5V8WAKDZ1Uf2",
        "outputId": "6ba2a680-a595-40e8-fa5d-5784b9614b97"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1;35mThread\u001b[0m\u001b[1m(\u001b[0m\n",
              "    \u001b[33mid\u001b[0m=\u001b[32m'thread_Yjm4riVN4Hb04PYXZuqwqHv7'\u001b[0m,\n",
              "    \u001b[33mcreated_at\u001b[0m=\u001b[1;36m1718401907\u001b[0m,\n",
              "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
              "    \u001b[33mobject\u001b[0m=\u001b[32m'thread'\u001b[0m,\n",
              "    \u001b[33mtool_resources\u001b[0m=\u001b[1;35mToolResources\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcode_interpreter\u001b[0m=\u001b[3;35mNone\u001b[0m, \u001b[33mfile_search\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m\n",
              "\u001b[1m)\u001b[0m"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "thread"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k6S_e501V4z"
      },
      "source": [
        "Notice some key attributes:\n",
        "\n",
        "- `id` - since each Thread is like a conversation, we need some way to specify which thread we're dealing with when interacting with them\n",
        "- `tool_resources` - this will become more relevant as we add tools since we'll need a way to verify which tools we have access to when interacting with our Assistant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5BvGv1N0c2h"
      },
      "source": [
        "### Adding Messages to Our Thread\n",
        "\n",
        "Now that we have our Thread (or conversation) we can start adding messages to it!\n",
        "\n",
        "Let's add a simple message that asks about how our Assistant is feeling.\n",
        "\n",
        "Notice the parameters we're leveraging:\n",
        "\n",
        "- `thread_id` - since each Thread is like a conversation, we need some way to address a specific conversation. We can use `thread.id` to do this.\n",
        "- `role` - similar to when we used our chat completions endpoint, this parameter specifies who the message is coming from. You can leverage this in the same ways you would through the chat completions endpoint.\n",
        "- `content` - this is where we can place the actual text our Assistant will interact with\n",
        "\n",
        "> NOTE: Feel free to substitute a relevant message based on the Assistant you created"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "R7ZNCfGivagg"
      },
      "outputs": [],
      "source": [
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id, role=\"user\", content=f\"How old are you?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc7R3Sr32P0b"
      },
      "source": [
        "Again, let's examine our `message` object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMLvyZDA2S_D",
        "outputId": "dfde63a0-3b6f-4f01-b76c-ec84baefe5a2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1;35mMessage\u001b[0m\u001b[1m(\u001b[0m\n",
              "    \u001b[33mid\u001b[0m=\u001b[32m'msg_OwONDWmC74Pq1op5uB8eqoZJ'\u001b[0m,\n",
              "    \u001b[33massistant_id\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[33mattachments\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
              "    \u001b[33mcompleted_at\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[33mcontent\u001b[0m=\u001b[1m[\u001b[0m\u001b[1;35mTextContentBlock\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtext\u001b[0m=\u001b[1;35mText\u001b[0m\u001b[1m(\u001b[0m\u001b[33mannotations\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mvalue\u001b[0m=\u001b[32m'How old are you?'\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mtype\u001b[0m=\u001b[32m'text'\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m,\n",
              "    \u001b[33mcreated_at\u001b[0m=\u001b[1;36m1718401909\u001b[0m,\n",
              "    \u001b[33mincomplete_at\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[33mincomplete_details\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
              "    \u001b[33mobject\u001b[0m=\u001b[32m'thread.message'\u001b[0m,\n",
              "    \u001b[33mrole\u001b[0m=\u001b[32m'user'\u001b[0m,\n",
              "    \u001b[33mrun_id\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[33mstatus\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[33mthread_id\u001b[0m=\u001b[32m'thread_Yjm4riVN4Hb04PYXZuqwqHv7'\u001b[0m\n",
              "\u001b[1m)\u001b[0m"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "message"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI3Pctpk29og"
      },
      "source": [
        "### Running Our Thread\n",
        "\n",
        "Now that we have an Assistant, and we've given that Assistant a Thread, and we've added a Message to that Thread - we're ready to run our Assistant!\n",
        "\n",
        "Notice that this process lets us add (potentially) multiple messages to our Assistant. We can leverage that behaviour for few/many-shot examples, and more!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "cellView": "form",
        "id": "VkvsXv5_3cyQ"
      },
      "outputs": [],
      "source": [
        "# @markdown #### üèóÔ∏è Build Activity üèóÔ∏è\n",
        "# @markdown We can also override the Assistant's instructions when we run a thread.\n",
        "\n",
        "# @markdown Use one of the [Prompt Principles for Instruction](https://arxiv.org/pdf/2312.16171v1.pdf) to improve the likeliehood of a correct or valuable response from your Assistant.\n",
        "\n",
        "additional_instructions = \"\"\"Your task is to return the description of an algorithm that can answer the user's question, explaining like they are 5 years old. \n",
        "Come up with a creative example. If no such algorithm can be made, say so. Do not generate code. Only generate 1 example.\"\"\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CuGbTrL5QEc"
      },
      "source": [
        "Let's run our Thread!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "fpWNl3UVvdW4"
      },
      "outputs": [],
      "source": [
        "run = client.beta.threads.runs.create(\n",
        "  thread_id=thread.id,\n",
        "  assistant_id=assistant.id,\n",
        "  instructions=additional_instructions\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erMGdU7y6la2"
      },
      "source": [
        "Now that we've run our thread, let's look at the object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kz_rfwi869YI",
        "outputId": "56aa670d-8f22-4ef4-b5f4-e568c6f7cefb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1;35mRun\u001b[0m\u001b[1m(\u001b[0m\n",
              "    \u001b[33mid\u001b[0m=\u001b[32m'run_R1qbwGhFYtPo03w1hJOlIL1N'\u001b[0m,\n",
              "    \u001b[33massistant_id\u001b[0m=\u001b[32m'asst_aSPvLTXMt0Z1bfUAVZcNTcO6'\u001b[0m,\n",
              "    \u001b[33mcancelled_at\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[33mcompleted_at\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[33mcreated_at\u001b[0m=\u001b[1;36m1718401917\u001b[0m,\n",
              "    \u001b[33mexpires_at\u001b[0m=\u001b[1;36m1718402517\u001b[0m,\n",
              "    \u001b[33mfailed_at\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[33mincomplete_details\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[33minstructions\u001b[0m=\u001b[32m\"Your\u001b[0m\u001b[32m task is to return the description of an algorithm that can answer the user's question, explaining like they are 5 years old. \\nCome up with a creative example. If no such algorithm can be made, say so. Do not generate code. Only generate 1 example.\"\u001b[0m,\n",
              "    \u001b[33mlast_error\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[33mmax_completion_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[33mmax_prompt_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
              "    \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o'\u001b[0m,\n",
              "    \u001b[33mobject\u001b[0m=\u001b[32m'thread.run'\u001b[0m,\n",
              "    \u001b[33mparallel_tool_calls\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
              "    \u001b[33mrequired_action\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[33mresponse_format\u001b[0m=\u001b[32m'auto'\u001b[0m,\n",
              "    \u001b[33mstarted_at\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[33mstatus\u001b[0m=\u001b[32m'queued'\u001b[0m,\n",
              "    \u001b[33mthread_id\u001b[0m=\u001b[32m'thread_Yjm4riVN4Hb04PYXZuqwqHv7'\u001b[0m,\n",
              "    \u001b[33mtool_choice\u001b[0m=\u001b[32m'auto'\u001b[0m,\n",
              "    \u001b[33mtools\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
              "    \u001b[33mtruncation_strategy\u001b[0m=\u001b[1;35mTruncationStrategy\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtype\u001b[0m=\u001b[32m'auto'\u001b[0m, \u001b[33mlast_messages\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m,\n",
              "    \u001b[33musage\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[33mtemperature\u001b[0m=\u001b[1;36m1\u001b[0m\u001b[1;36m.0\u001b[0m,\n",
              "    \u001b[33mtop_p\u001b[0m=\u001b[1;36m1\u001b[0m\u001b[1;36m.0\u001b[0m,\n",
              "    \u001b[33mtool_resources\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
              "\u001b[1m)\u001b[0m"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h2SH_347JJb"
      },
      "source": [
        "Notice we have access to a few very powerful parameters in this `run` object.\n",
        "\n",
        "- `completed_at` - this will help us determine when we can expect to retrieve a response\n",
        "- `failed_at` - this can highlight any issues our run ran into\n",
        "- `status` - is another way we can understand how the flow is going"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVBNagBU7kpx"
      },
      "source": [
        "### Retrieving Our Run\n",
        "\n",
        "Now that we've created our run, let's retrieve it.\n",
        "\n",
        "We're going to wrap this in a simple loop to make sure we're not retrieving it too early."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "itz5_otPvfkV"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "while run.status == \"in_progress\" or run.status == \"queued\":\n",
        "    time.sleep(1)\n",
        "    run = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgGE1uUJ7z3h",
        "outputId": "89f3df27-d5a3-4095-9422-6271db52cfba"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">completed\n",
              "</pre>\n"
            ],
            "text/plain": [
              "completed\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(run.status)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHVTS4hD7-fv"
      },
      "source": [
        "Now that our run is completed - we can retieve the messages from our thread!\n",
        "\n",
        "Notice that our run helps us understand how things are going - but it isn't where we're going to find our responses or messages. Those are added on the backend into our thread.\n",
        "\n",
        "This leads to a simple, but important, flow:\n",
        "\n",
        "1. We add messages to a thread.\n",
        "2. We create a run on that thread.\n",
        "3. We wait until the run is finished.\n",
        "4. We check our thread for the new messages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGBNpGmh-ZpW"
      },
      "source": [
        "### Checking Our Thread\n",
        "\n",
        "Now we can get a list of messages from our thread!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Av-OQDUPvhAd"
      },
      "outputs": [],
      "source": [
        "messages = client.beta.threads.messages.list(thread_id=thread.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM6cZk-GviqX",
        "outputId": "3916ed59-7eba-471f-9eb7-d8d2f4658012"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1;35mMessage\u001b[0m\u001b[1m(\u001b[0m\n",
              "    \u001b[33mid\u001b[0m=\u001b[32m'msg_AKbjeemGk7xFaFQuw2PwRqOK'\u001b[0m,\n",
              "    \u001b[33massistant_id\u001b[0m=\u001b[32m'asst_aSPvLTXMt0Z1bfUAVZcNTcO6'\u001b[0m,\n",
              "    \u001b[33mattachments\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
              "    \u001b[33mcompleted_at\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[33mcontent\u001b[0m=\u001b[1m[\u001b[0m\n",
              "        \u001b[1;35mTextContentBlock\u001b[0m\u001b[1m(\u001b[0m\n",
              "            \u001b[33mtext\u001b[0m=\u001b[1;35mText\u001b[0m\u001b[1m(\u001b[0m\n",
              "                \u001b[33mannotations\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
              "                \u001b[33mvalue\u001b[0m=\u001b[32m\"Imagine\u001b[0m\u001b[32m we have a magical age-guessing robot named Timmy. Timmy is very smart and good at numbers! But Timmy can't tell you his age directly because he's a robot and doesn't age like humans do.\\n\\nInstead, let's play a little game. Think of how old you are, and then imagine that Timmy's age is just like a very wise, grown-up helper in a toy shop. Timmy has been helping kids for as long as you can imagine, maybe even forever in a toy land where nobody keeps track of years. So, Timmy's age is a mystery, just like magic!\\n\\nNow, whenever you ask Timmy how old he is, he wants you to think of him as someone who is always there to help, no matter the number of years. So, while Timmy might not give you an exact age, you can think of him as your always-there, super-wise friend in that wonderful toy shop. Isn't that fun?\"\u001b[0m\n",
              "            \u001b[1m)\u001b[0m,\n",
              "            \u001b[33mtype\u001b[0m=\u001b[32m'text'\u001b[0m\n",
              "        \u001b[1m)\u001b[0m\n",
              "    \u001b[1m]\u001b[0m,\n",
              "    \u001b[33mcreated_at\u001b[0m=\u001b[1;36m1718401917\u001b[0m,\n",
              "    \u001b[33mincomplete_at\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[33mincomplete_details\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
              "    \u001b[33mobject\u001b[0m=\u001b[32m'thread.message'\u001b[0m,\n",
              "    \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
              "    \u001b[33mrun_id\u001b[0m=\u001b[32m'run_R1qbwGhFYtPo03w1hJOlIL1N'\u001b[0m,\n",
              "    \u001b[33mstatus\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[33mthread_id\u001b[0m=\u001b[32m'thread_Yjm4riVN4Hb04PYXZuqwqHv7'\u001b[0m\n",
              "\u001b[1m)\u001b[0m"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "messages.data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT9_V_XlaVH7"
      },
      "source": [
        "### Streaming Our Runs\n",
        "\n",
        "With recent upgrades to the Assistant API - we can now *stream* our outputs!\n",
        "\n",
        "In order to do this - we'll need something called an `EventHandler` which will help us to decide on what actions to take based on the output of the LLM.\n",
        "\n",
        "Let's build it below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "YvH-FUbQawcN"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import override\n",
        "from openai import AssistantEventHandler\n",
        "from sys import stdout\n",
        "\n",
        "stdout.flush()\n",
        "\n",
        "class EventHandler(AssistantEventHandler):\n",
        "    @override\n",
        "    def on_text_created(self, text) -> None:\n",
        "        stdout.write(f\"\\nassistant > \")\n",
        "\n",
        "    @override\n",
        "    def on_text_delta(self, delta, snapshot):\n",
        "        stdout.write(delta.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvBieRYvbAmt"
      },
      "source": [
        "Now we can create our `run` and stream the output as it comes in!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8ax_0ZfbF_I",
        "outputId": "1bbddea1-a145-4a0d-b662-73f7703a511f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "assistant > Imagine we have a magical garden with a special tree named Treebot. Treebot is very, very old and wise, and it knows everything about taking care of the garden. But, just like with magic trees, we don't really count how old Treebot is in the same way we count birthdays for kids.\n",
            "\n",
            "Instead, let's say Treebot's age is like the number of beautiful flowers it has helped grow in the garden. Suppose every year it helps a new batch of flowers bloom beautifully. If you see many flowers, then you know Treebot has been there for a long time!\n",
            "\n",
            "So, when you ask Treebot how old it is, imagine it has been there forever, always helping each flower grow happily. The exact number of years doesn't matter because Treebot's job is to make the garden wonderful all the time. That way, you can always think of Treebot as your forever-friend who loves making the garden pretty for everyone."
          ]
        }
      ],
      "source": [
        "with client.beta.threads.runs.stream(\n",
        "    thread_id=thread.id,\n",
        "    assistant_id=assistant.id,\n",
        "    instructions=additional_instructions,\n",
        "    event_handler=EventHandler(),\n",
        ") as stream:\n",
        "    stream.until_done()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgnY16tjCmc6"
      },
      "source": [
        "## Task 2: Adding Tools\n",
        "\n",
        "Now that we have an understanding of how Assistant works, we can start thinking about adding tools.\n",
        "\n",
        "We'll go through 3 separate tools and explore how we can leverage them!\n",
        "\n",
        "Let's start with the most familiar tool - the Retriever!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0NagnlZC8g9"
      },
      "source": [
        "### Task 2a: Creating an Assistant with the File Search Tool\n",
        "\n",
        "The first thing we'll want to do is create an assistant with the File Search tool.\n",
        "\n",
        "This is also going to require some data. We'll provided data - but you're very much encouraged to use your own files to explore how the Assistant works for your use case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HInYwNiQEjQH"
      },
      "source": [
        "#### Collect and Add Data to Vector Store\n",
        "\n",
        "1. First, we need some data.\n",
        "2. Second, we need to add the data to our Assistant!\n",
        "\n",
        "Let's start with grabbing some data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvAHBszIEa1Y",
        "outputId": "c9e86f6b-89b5-426b-ecd8-a6d23cbea92b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0 3003k    0 23999    0     0  80432      0  0:00:38 --:--:--  0:00:38 81907\n",
            "100 3003k  100 3003k    0     0  5857k      0 --:--:-- --:--:-- --:--:-- 5924k\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://raw.githubusercontent.com/dbredvick/paul-graham-to-kindle/main/paul_graham_essays.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2EpY1w_FQ3m"
      },
      "source": [
        "Now we can upload our file to our Vector Store!\n",
        "\n",
        "Pay attention to [this](https://platform.openai.com/docs/assistants/tools/file-search/supported-files) documentation to see what kinds of files can be uploaded.\n",
        "\n",
        "> NOTE: Per the OpenAI [docs](https://platform.openai.com/docs/assistants/tools/file-search/vector-stores) The maximum file size is 512 MB and no more than 2,000,000 tokens (computed automatically when you attach a file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "6UgUYNTaOpUK"
      },
      "outputs": [],
      "source": [
        "vector_store = client.beta.vector_stores.create(name=\"Paul Graham Essay Compilation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "dpVoe2SMFI6s"
      },
      "outputs": [],
      "source": [
        "file_paths = [\"paul_graham_essays.txt\"]\n",
        "file_streams = [open(path, \"rb\") for path in file_paths]\n",
        "\n",
        "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
        "    vector_store_id=vector_store.id, files=file_streams\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBQOSGyyF2u5"
      },
      "source": [
        "Let's look at what our `file_batch` contains!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "zJrVLkMpFgwf"
      },
      "outputs": [],
      "source": [
        "while file_batch.status != \"completed\":\n",
        "  time(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd4O4dpZF-eH"
      },
      "source": [
        "#### Create and Use Assistant\n",
        "\n",
        "Now that we have our file - we can attach it to an Assistant, and we can give that Assistant the ability to use it for retrieval through the Retrieval tool!\n",
        "\n",
        "> NOTE: Your first GB is free and beyond that, usage is billed at $0.10/GB/day of vector storage. There are no other costs associated with vector store operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "jfn_MlJqFiEe"
      },
      "outputs": [],
      "source": [
        "fs_assistant = client.beta.assistants.create(\n",
        "    name=name,\n",
        "    instructions=instructions,\n",
        "    model=model,\n",
        "    tools=[{\"type\": \"file_search\"}],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "AhXXOXp1eybd"
      },
      "outputs": [],
      "source": [
        "fs_assistant = client.beta.assistants.update(\n",
        "    assistant_id=fs_assistant.id,\n",
        "    tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1;35mAssistant\u001b[0m\u001b[1m(\u001b[0m\n",
              "    \u001b[33mid\u001b[0m=\u001b[32m'asst_hJ4ZpUpPWAfHAfovKoeZxknz'\u001b[0m,\n",
              "    \u001b[33mcreated_at\u001b[0m=\u001b[1;36m1718402090\u001b[0m,\n",
              "    \u001b[33mdescription\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[33minstructions\u001b[0m=\u001b[32m'Answer in a professional manner.'\u001b[0m,\n",
              "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
              "    \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o'\u001b[0m,\n",
              "    \u001b[33mname\u001b[0m=\u001b[32m'General Chatbot'\u001b[0m,\n",
              "    \u001b[33mobject\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
              "    \u001b[33mtools\u001b[0m=\u001b[1m[\u001b[0m\u001b[1;35mFileSearchTool\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtype\u001b[0m=\u001b[32m'file_search'\u001b[0m, \u001b[33mfile_search\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m,\n",
              "    \u001b[33mresponse_format\u001b[0m=\u001b[32m'auto'\u001b[0m,\n",
              "    \u001b[33mtemperature\u001b[0m=\u001b[1;36m1\u001b[0m\u001b[1;36m.0\u001b[0m,\n",
              "    \u001b[33mtool_resources\u001b[0m=\u001b[1;35mToolResources\u001b[0m\u001b[1m(\u001b[0m\n",
              "        \u001b[33mcode_interpreter\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "        \u001b[33mfile_search\u001b[0m=\u001b[1;35mToolResourcesFileSearch\u001b[0m\u001b[1m(\u001b[0m\u001b[33mvector_store_ids\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'vs_ERIwlgjBbjCdMHUonq5I2PSc'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
              "    \u001b[1m)\u001b[0m,\n",
              "    \u001b[33mtop_p\u001b[0m=\u001b[1;36m1\u001b[0m\u001b[1;36m.0\u001b[0m\n",
              "\u001b[1m)\u001b[0m"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fs_assistant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "JM9iJhoMcUfW"
      },
      "outputs": [],
      "source": [
        "fs_thread = client.beta.threads.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What did Paul Graham say about effective altruism?\",\n",
        "        }\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0-mmRjQGeUR"
      },
      "source": [
        "We can use an extension of the `EventHandler` that we created above to stream our `run`!\n",
        "\n",
        "Let's add a few things:\n",
        "\n",
        "1. A `on_tool_call_created` function which tells us which tool is being used.\n",
        "2. A `on_message_done` that includes citations that were used by our File Search tool - this is like return the context *and* the response that we saw in our Pythonic RAG implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "av5A_tm0bpvf"
      },
      "outputs": [],
      "source": [
        "stdout.flush()\n",
        "\n",
        "\n",
        "class FSEventHandler(AssistantEventHandler):\n",
        "    @override\n",
        "    def on_text_created(self, text) -> None:\n",
        "        stdout.write(f\"\\nassistant > \")\n",
        "\n",
        "    @override\n",
        "    def on_tool_call_created(self, tool_call):\n",
        "        stdout.write(f\"\\nassistant > {tool_call.type}\\n\")\n",
        "\n",
        "    @override\n",
        "    def on_message_done(self, message) -> None:\n",
        "        message_content = message.content[0].text\n",
        "        annotations = message_content.annotations\n",
        "        citations = []\n",
        "        for index, annotation in enumerate(annotations):\n",
        "            message_content.value = message_content.value.replace(\n",
        "                annotation.text, f\"[{index}]\"\n",
        "            )\n",
        "            if file_citation := getattr(annotation, \"file_citation\", None):\n",
        "                cited_file = client.files.retrieve(file_citation.file_id)\n",
        "                citations.append(f\"[{index}] {cited_file.filename}\")\n",
        "\n",
        "        stdout.write(message_content.value + \"\\n\")\n",
        "        stdout.write(\"\\n\".join(citations) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxJfa-saHbNQ"
      },
      "source": [
        "Let's look at the final result!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19nDqjRgHaNA",
        "outputId": "b33c4c2d-e8a7-432c-b596-a2169c487f9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "assistant > file_search\n",
            "\n",
            "assistant > Paul Graham has expressed a skeptical view of effective altruism. He criticizes it on several points: he feels that it \"makes a parody of something important\" and that it \"causes people to wear themselves out doing things that don't matter.\" He does not conclude definitively whether effective altruism is net good or bad, but he leans towards the idea that it might be more likely bad[0][1].\n",
            "[0] paul_graham_essays.txt\n",
            "[1] paul_graham_essays.txt\n"
          ]
        }
      ],
      "source": [
        "with client.beta.threads.runs.stream(\n",
        "    thread_id=fs_thread.id,\n",
        "    assistant_id=fs_assistant.id,\n",
        "    event_handler=FSEventHandler(),\n",
        ") as stream:\n",
        "    stream.until_done()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pln9uYoJICno"
      },
      "source": [
        "### Task 2b: Creating an Assistant with the Code Interpreter Tool\n",
        "\n",
        "Now that we've explored the Retrieval Tool - let's try the Code Interpreter tool!\n",
        "\n",
        "The process will be almost exactly the same - but we can explore a different query, and we'll add our file at the Message level!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "IC81y_VtH9lw"
      },
      "outputs": [],
      "source": [
        "ci_assistant = client.beta.assistants.create(\n",
        "    name=name + \"+ Code Interpreter\",\n",
        "    instructions=instructions,\n",
        "    model=model,\n",
        "    tools=[{\"type\": \"code_interpreter\"}],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "7IPHZswUg6RH"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'datasets'...\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ali-ce/datasets.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "_a8BLVdog1X8"
      },
      "outputs": [],
      "source": [
        "file = client.files.create(\n",
        "    file=open(\"datasets/Y-Combinator/Startups.csv\", \"rb\"), purpose=\"assistants\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJPHAJCQJbgi"
      },
      "source": [
        "In the following example, we'll also see how we can package the Thread creation with the Message adding step!\n",
        "\n",
        "> NOTE: Files added at the message/thread level will not be available to the Assistant outside of that Thread."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "5xVdjH6EJQrr"
      },
      "outputs": [],
      "source": [
        "ci_thread = client.beta.threads.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What kind of file is this?\",\n",
        "            \"attachments\": [\n",
        "                {\"file_id\": file.id, \"tools\": [{\"type\": \"code_interpreter\"}]}\n",
        "            ],\n",
        "        }\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiIYut_dJ0Cv"
      },
      "source": [
        "We'll once again need to create an `EventHandler`, except this time it will have an `on_tool_call_delta` method which will let us see the output of the code interpreter tool as well!\n",
        "\n",
        "> NOTE: Remember that we create runs at the *thread* level - and so don't need the message object to continue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "or7iJ492KI2P"
      },
      "outputs": [],
      "source": [
        "stdout.flush()\n",
        "\n",
        "\n",
        "class CIEventHandler(AssistantEventHandler):\n",
        "    @override\n",
        "    def on_text_created(self, text) -> None:\n",
        "        stdout.write(f\"\\nassistant > \")\n",
        "\n",
        "    @override\n",
        "    def on_text_delta(self, delta, snapshot):\n",
        "        stdout.write(delta.value)\n",
        "\n",
        "    def on_tool_call_created(self, tool_call):\n",
        "        stdout.write(f\"\\nassistant > {tool_call.type}\\n\")\n",
        "\n",
        "    def on_tool_call_delta(self, delta, snapshot):\n",
        "        if delta.type == \"code_interpreter\":\n",
        "            if delta.code_interpreter.input:\n",
        "                stdout.write(delta.code_interpreter.input)\n",
        "            if delta.code_interpreter.outputs:\n",
        "                stdout.write(f\"\\n\\noutput >\")\n",
        "                for output in delta.code_interpreter.outputs:\n",
        "                    if output.type == \"logs\":\n",
        "                        stdout.write(f\"\\n{output.logs}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPiivot-n5A8"
      },
      "source": [
        "Once again, we can use the streaming interface thanks to creating our `EventHandler`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXsmBqFfiSvT",
        "outputId": "dbfed50c-4e66-4a01-e36a-9651be586be0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "assistant > code_interpreter\n",
            "import mimetypes\n",
            "\n",
            "# File path\n",
            "file_path = '/mnt/data/file-Sa9K7FjxfuUaXFUXBeJg7LSv'\n",
            "\n",
            "# Determine the file type\n",
            "file_type, _ = mimetypes.guess_type(file_path)\n",
            "file_type\n",
            "assistant > I can't tell what the file is just by its name. Let me take a closer look to see if I can figure it out for you. One second!# Let's try to read the first few bytes of the file to identify its type\n",
            "with open(file_path, 'rb') as file:\n",
            "    file_header = file.read(20)\n",
            "file_header\n",
            "assistant > Alright, imagine you have a magical book. When you open the first page, you see that it's talking about different companies, their status, and the year. So, from the first few words, it looks like the file is some kind of list or table about companies. This type of file is often called a CSV file, which stands for Comma-Separated Values. It's like a big table with lots of information organized neatly into rows and columns."
          ]
        }
      ],
      "source": [
        "with client.beta.threads.runs.stream(\n",
        "    thread_id=ci_thread.id,\n",
        "    assistant_id=ci_assistant.id,\n",
        "    instructions=additional_instructions,\n",
        "    event_handler=CIEventHandler(),\n",
        ") as stream:\n",
        "    stream.until_done()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi10hON2LQmc"
      },
      "source": [
        "And there you go!\n",
        "\n",
        "We've fit our Assistant with an awesome Code Interpreter that lets our Assistant run code on our provided files!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdJxt77oLzu7"
      },
      "source": [
        "### Task 2c: Creating an Assistant with a Function Calling Tool\n",
        "\n",
        "Let's finally create an Assistant that utilizes the Function Calling API.\n",
        "\n",
        "We'll start by creating a function that we wish to be called.\n",
        "\n",
        "We'll utilize DuckDuckGo search to allow our Assistant to have the most up to date information!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5eKEC2wMMVI",
        "outputId": "1712c586-0c40-410c-e2e6-07ea7c918c54"
      },
      "outputs": [],
      "source": [
        "!pip install -qU duckduckgo_search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "YPFZ_Uq_LawH"
      },
      "outputs": [],
      "source": [
        "from duckduckgo_search import DDGS\n",
        "\n",
        "\n",
        "def duckduckgo_search(query):\n",
        "    with DDGS() as ddgs:\n",
        "        results = [r for r in ddgs.text(query, max_results=5)]\n",
        "        return \"\\n\".join(result[\"body\"] for result in results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-o1TBFpMSvR"
      },
      "source": [
        "Let's test our function to make sure it behaves as we expect it to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "mCUr9jFCMWBw",
        "outputId": "ea9bb779-f7c3-49e5-b5d4-c12be875a460"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "\u001b[32m\"Chief Executive Officer. Tim Cook is the CEO of Apple and serves on its board of directors. Before being named CEO in August 2011, Tim was Apple's chief operating officer and was responsible for all of the company's worldwide sales and operations, including end-to-end management of Apple's supply chain, sales activities, and service and ...\\nApple Leadership - Apple Executive Profiles is a webpage that introduces the top executives of Apple, the world's most innovative company. You can learn about their backgrounds, roles, and achievements, as well as their vision and values for Apple. Whether you are a fan, a customer, or a potential partner, you will find this webpage informative and inspiring.\\nWebsite. Apple Leadership Profile. Signature. Timothy Donald Cook \u001b[0m\u001b[32m(\u001b[0m\u001b[32mborn November 1, 1960\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\u001b[32m[\u001b[0m\u001b[32m1\u001b[0m\u001b[32m]\u001b[0m\u001b[32m is an American business executive who is the current chief executive officer of Apple Inc. Cook had previously been the company's chief operating officer under its co-founder Steve Jobs. \u001b[0m\u001b[32m[\u001b[0m\u001b[32m2\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Cook joined Apple in March 1998 as a senior vice president ...\\nTim Cook is the CEO of Apple, one of the world's most valuable companies. Cook, who became chief executive in 2011, had previously served as Apple's chief operating officer under Steve Jobs.\\nTim Cook is the chief executive officer of Apple since 2011, after Steve Jobs' death. He joined Apple in 1998 and led the company's turnaround, as well as its acquisitions of Beats and other products.\"\u001b[0m"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "duckduckgo_search(\"Who is the CEO of Apple?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzE1nxt5Mi80"
      },
      "source": [
        "Now we need to express how our function works in a way that is compatible with the OpenAI Function Calling API.\n",
        "\n",
        "We'll want to provide a `JSON` object that includes what parameters we have, how to call them, and a short natural language description."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "8ElrWvBnMY_s"
      },
      "outputs": [],
      "source": [
        "ddg_function = {\n",
        "    \"name\": \"duckduckgo_search\",\n",
        "    \"description\": \"Answer non-technical questions. \",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"query\": {\n",
        "                \"type:\": \"string\",\n",
        "                \"description\": \"The search query to use. For example: 'Who is the current Goalie of the Colorado Avalance?'\",\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"query\"],\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyRmJgEQVuGs"
      },
      "source": [
        "#### ‚ùì Question\n",
        "\n",
        "Why does the description key-value pair matter?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ‚ùì Answer\n",
        "\n",
        "The `description` key-value pair is important because it helps the Assistant understand what the function does and how it can be used. You can think of this as being similar to a function's docstring that helps developers understand how to use the function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tir4WySGM0x2"
      },
      "source": [
        "Now when we create our Assistant - we'll want to include the function description as a tool using the following format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "4eFpwi12Mzlg"
      },
      "outputs": [],
      "source": [
        "fc_assistant = client.beta.assistants.create(\n",
        "    name=name + \" + Function Calling\",\n",
        "    instructions=instructions,\n",
        "    tools=[{\"type\": \"function\", \"function\": ddg_function}],\n",
        "    model=model,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcoiF_gXksaa"
      },
      "source": [
        "Now we can create our thread, and attach our message to it - just as we've been doing!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "CtELZ5wFjdo_"
      },
      "outputs": [],
      "source": [
        "fc_thread = client.beta.threads.create()\n",
        "fc_message = client.beta.threads.messages.create(\n",
        "    thread_id=fc_thread.id,\n",
        "    role=\"user\",\n",
        "    content=\"Can you describe the Twitter beef between Elon and Sam Altman?\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KT_dj2YkwyK"
      },
      "source": [
        "Once again, we'll need an `EventHandler` for the streaming response - notice that this time we're utilizing a few new methods:\n",
        "\n",
        "1. `handle_requires_action` - this will handle whatever action needs to take place in *our local environment*.\n",
        "2. `submit_tool_outputs` - this will let us submit the resultant outputs from our local function call back to the Assistant run in the required format.\n",
        "\n",
        "To be very clear and explicit - we'll be following this pattern:\n",
        "\n",
        "1. Make a call to the LLM which will decide if a local function call is required.\n",
        "2. If a local function call is required - send a response that indicates a local function call is required.\n",
        "3. Call the function using the arguments provided by the LLM.\n",
        "4. Return the response from the function call with LLM provided arguements.\n",
        "5. Receive a response based on the output of the functional call from the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "qgXhMEHbizEX"
      },
      "outputs": [],
      "source": [
        "class FCEventHandler(AssistantEventHandler):\n",
        "    @override\n",
        "    def on_event(self, event):\n",
        "        # Retrieve events that are denoted with 'requires_action'\n",
        "        # since these will have our tool_calls\n",
        "        if event.event == \"thread.run.requires_action\":\n",
        "            run_id = event.data.id  # Retrieve the run ID from the event data\n",
        "            self.handle_requires_action(event.data, run_id)\n",
        "\n",
        "    def handle_requires_action(self, data, run_id):\n",
        "        tool_outputs = []\n",
        "\n",
        "        for tool in data.required_action.submit_tool_outputs.tool_calls:\n",
        "            print(tool.function.arguments)\n",
        "            if tool.function.name == \"duckduckgo_search\":\n",
        "                tool_outputs.append(\n",
        "                    {\n",
        "                        \"tool_call_id\": tool.id,\n",
        "                        \"output\": duckduckgo_search(tool.function.arguments),\n",
        "                    }\n",
        "                )\n",
        "\n",
        "        # Submit all tool_outputs at the same time\n",
        "        self.submit_tool_outputs(tool_outputs, run_id)\n",
        "\n",
        "    def submit_tool_outputs(self, tool_outputs, run_id):\n",
        "        # Use the submit_tool_outputs_stream helper\n",
        "        with client.beta.threads.runs.submit_tool_outputs_stream(\n",
        "            thread_id=self.current_run.thread_id,\n",
        "            run_id=self.current_run.id,\n",
        "            tool_outputs=tool_outputs,\n",
        "            event_handler=EventHandler(),\n",
        "        ) as stream:\n",
        "            stream.until_done()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9Gzz4OZnxCQ"
      },
      "source": [
        "Thanks to our event handler - we can stream this process in our notebook!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HGInY8fjYNa",
        "outputId": "b6fc88c6-4e58-46f2-c21a-0869fc208c46"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"query\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Elon Musk and Sam Altman Twitter beef\"</span><span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m{\u001b[0m\u001b[32m\"query\"\u001b[0m: \u001b[32m\"Elon Musk and Sam Altman Twitter beef\"\u001b[0m\u001b[1m}\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"query\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Elon Musk Sam Altman conflict details\"</span><span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m{\u001b[0m\u001b[32m\"query\"\u001b[0m: \u001b[32m\"Elon Musk Sam Altman conflict details\"\u001b[0m\u001b[1m}\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "assistant > It appears that there might not be widely known or documented recent \"Twitter beef\" or conflict between Elon Musk and Sam Altman. Both are prominent figures in the tech industry, Musk as the CEO of companies like Tesla and SpaceX, and Altman as the CEO of OpenAI. They have been known to have their differences, especially in perspectives on artificial intelligence and its implications.\n",
            "\n",
            "Elon Musk has been a vocal critic of the potential dangers of AI, advocating for strict regulation to ensure it is developed safely. Sam Altman, on the other hand, as the head of OpenAI, promotes the development of AI but also recognizes the need for safety and ethical guidelines. Their differing views could potentially lead to public disagreements.\n",
            "\n",
            "However, without specific recent information about a direct conflict or \"Twitter beef\" between the two, it's important to consider that their public interactions might be more rooted in professional debates rather than personal animosity. If you have seen something specific, providing more context would help in digging deeper into their interactions."
          ]
        }
      ],
      "source": [
        "stdout.flush()\n",
        "with client.beta.threads.runs.stream(\n",
        "    thread_id=fc_thread.id, assistant_id=fc_assistant.id, event_handler=FCEventHandler()\n",
        ") as stream:\n",
        "    stream.until_done()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
